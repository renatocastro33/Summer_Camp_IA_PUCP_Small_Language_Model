{"cells":[{"cell_type":"markdown","id":"f01e1d45-3aab-465c-9480-fc7dc5106e69","metadata":{"panel-layout":{"height":60.59375,"visible":true,"width":100},"id":"f01e1d45-3aab-465c-9480-fc7dc5106e69"},"source":["# Small Language Workshop Part 1"]},{"cell_type":"markdown","id":"306cd8b2-2a76-422a-b78c-f181e1a64620","metadata":{"panel-layout":{"height":220.421875,"visible":true,"width":100},"id":"306cd8b2-2a76-422a-b78c-f181e1a64620"},"source":["# Tokenization from Scratch: Building a Byte Pair Encoder (BPE)\n","\n","In this notebook, we will implement a **byte-level tokenizer** similar to the one used in GPT-style models.\n","\n","By the end, you will:\n","- Understand why tokenization is necessary\n","- Implement Byte Pair Encoding from scratch\n","- Train a tokenizer on real Unicode text\n","- Encode and decode text losslessly\n","- Understand why tokenization matters for Small Language Models"]},{"cell_type":"markdown","id":"bdae7223-9d36-4c45-93a2-bf4fd6a74bcd","metadata":{"panel-layout":{"height":223.234375,"visible":true,"width":100},"id":"bdae7223-9d36-4c45-93a2-bf4fd6a74bcd"},"source":["## 1. Why Tokenization?\n","\n","Neural networks operate on **numbers**, but language is **text**.\n","\n","Key challenges:\n","- Variable-length sequences\n","- Unicode & emojis\n","- Finite vocabulary\n","- Efficiency (context length matters!)\n","\n","ğŸ’¡ **Key idea:** Tokenization is a *compression problem*."]},{"cell_type":"markdown","id":"7aa9107a-b677-4b2a-aab2-a18757cce42b","metadata":{"id":"7aa9107a-b677-4b2a-aab2-a18757cce42b"},"source":["<img src='Neural_Network.png'>"]},{"cell_type":"markdown","id":"b64c1823-9a51-408f-81b5-3adf864b17d8","metadata":{"id":"b64c1823-9a51-408f-81b5-3adf864b17d8"},"source":["## 2. Text â†’ Bytes\n","\n","Instead of characters or words, GPT-style models operate on **bytes**.\n","\n","Why?\n","- Every string can be represented as bytes\n","- No unknown tokens\n","- Unicode-safe"]},{"cell_type":"code","execution_count":null,"id":"09e5f92a-3350-4be8-b11d-6162efcfda79","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09e5f92a-3350-4be8-b11d-6162efcfda79","executionInfo":{"status":"ok","timestamp":1768197536138,"user_tz":300,"elapsed":32,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"409d14db-2b84-403f-fed6-7d79279959af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original text: hello ğŸ˜„ students\n","UTF-8 Encoding: b'hello \\xf0\\x9f\\x98\\x84 students'\n","List Encoded UTF-8: [104, 101, 108, 108, 111, 32, 240, 159, 152, 132, 32, 115, 116, 117, 100, 101, 110, 116, 115]\n"]}],"source":["text = \"hello ğŸ˜„ students\"\n","encoded = text.encode(\"utf-8\")\n","\n","print(\"Original text:\", text)\n","print(\"UTF-8 Encoding:\", encoded) # Bytes values\n","print(\"List Encoded UTF-8:\", list(encoded)) # Decimal Values in Hexadecimal"]},{"cell_type":"markdown","id":"412d4f8c-61d0-44c4-8b7d-8ffbda3ae31d","metadata":{"id":"412d4f8c-61d0-44c4-8b7d-8ffbda3ae31d"},"source":["ğŸ§  **Think**\n","- Why does the emoji produce multiple numbers?\n","- Why might this be better than character-level tokenization?"]},{"cell_type":"markdown","id":"7ee7b3d8-be11-48b0-8327-87e2e91fecf8","metadata":{"id":"7ee7b3d8-be11-48b0-8327-87e2e91fecf8"},"source":["## 3. Initial Vocabulary\n","\n","We start with a vocabulary of **256 tokens**, one for each possible byte."]},{"cell_type":"code","execution_count":null,"id":"6b970630-c84a-4f9c-887e-83512313fe3b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6b970630-c84a-4f9c-887e-83512313fe3b","executionInfo":{"status":"ok","timestamp":1768197538859,"user_tz":300,"elapsed":15,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"629a151f-f9a7-45e7-e8e5-bfe51537e4f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["b'a' a\n","b'h' b'h'\n","b'\\xf0' b'\\xf0'\n"]}],"source":["vocab = {i: bytes([i]) for i in range(256)}\n","\n","# Sanity check\n","print(vocab[97], vocab[97].decode(\"utf-8\"))  # 'a'\n","\n","# So we can take the previous values of the string \"hello ğŸ˜„ students\"\n","print(vocab[104], bytes([encoded[0]]))  # It should match\n","print(vocab[240], bytes([encoded[6]]))  # It should match"]},{"cell_type":"markdown","id":"74347d31-bfd2-414e-890e-ff13222be5dc","metadata":{"id":"74347d31-bfd2-414e-890e-ff13222be5dc"},"source":["## 4. Counting Adjacent Pairs\n","\n","Byte Pair Encoding works by repeatedly merging the **most frequent adjacent pair**."]},{"cell_type":"code","execution_count":null,"id":"0560b4d0-8fd9-4167-84ce-6c4dc2b62570","metadata":{"id":"0560b4d0-8fd9-4167-84ce-6c4dc2b62570"},"outputs":[],"source":["def get_pair(ids):\n","    counts = {}\n","    for pair in zip(ids, ids[1:]):\n","        # TODO: count how many times each pair appears\n","\n","    return counts"]},{"cell_type":"code","execution_count":null,"id":"c72a792a-e430-497b-ba57-54495569e0a3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c72a792a-e430-497b-ba57-54495569e0a3","executionInfo":{"status":"ok","timestamp":1768197542326,"user_tz":300,"elapsed":11,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"f8f40523-56fc-4e47-b577-063acdae42c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["{(1, 2): 2, (2, 3): 1, (3, 1): 1}\n"]}],"source":["ids = [1, 2, 3, 1, 2]\n","print(get_pair(ids))\n","\n","# (1,1) or (2,2) does not count because the elements are equal"]},{"cell_type":"markdown","id":"b3c97e5e-83bf-47d9-9240-e70ae279d7a5","metadata":{"id":"b3c97e5e-83bf-47d9-9240-e70ae279d7a5"},"source":["## 5. Merging Pairs\n","\n","When we merge a pair `(a, b)` into a new token `k`,\n","we replace all occurrences of `(a, b)` with `k`."]},{"cell_type":"markdown","id":"329cc33f-251c-4b8a-b180-6ecd6f90a87f","metadata":{"id":"329cc33f-251c-4b8a-b180-6ecd6f90a87f"},"source":["<img src = 'BPE_Algorithm.png'>"]},{"cell_type":"code","execution_count":null,"id":"62ee8903-1912-45f5-8749-ec4c084fb5f0","metadata":{"id":"62ee8903-1912-45f5-8749-ec4c084fb5f0"},"outputs":[],"source":["def merge(ids, pair, index):\n","    new_ids = []\n","    i = 0\n","    while i < len(ids):\n","        if i < len(ids) - 1 and (ids[i], ids[i+1]) == pair:\n","            new_ids.append(index)\n","            i += 2\n","        else:\n","            new_ids.append(ids[i])\n","            i += 1\n","    return new_ids"]},{"cell_type":"code","execution_count":null,"id":"ed48a2a6-fe23-4fe8-a16c-87303ab69ba9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ed48a2a6-fe23-4fe8-a16c-87303ab69ba9","executionInfo":{"status":"ok","timestamp":1768197554879,"user_tz":300,"elapsed":18,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"9074f900-21ca-4be9-931a-64c7598f0a83"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[256, 3, 256]"]},"metadata":{},"execution_count":6}],"source":["merge([1, 2, 3, 1, 2], (1, 2), 256)"]},{"cell_type":"markdown","id":"124acf4b-018c-4d36-a3dc-6c87fe002542","metadata":{"id":"124acf4b-018c-4d36-a3dc-6c87fe002542"},"source":["What happened to sequence length? Why is this useful?"]},{"cell_type":"markdown","id":"162fa746-9ecf-4a1b-b560-acb1f9ad42f6","metadata":{"id":"162fa746-9ecf-4a1b-b560-acb1f9ad42f6"},"source":["## 6. Training the Tokenizer\n","\n","We will now repeatedly:\n","1. Count all adjacent pairs\n","2. Select the most frequent one\n","3. Merge it into a new token"]},{"cell_type":"code","execution_count":null,"id":"2bc14afb-3418-4758-8987-ec6e3bf69c97","metadata":{"id":"2bc14afb-3418-4758-8987-ec6e3bf69c97"},"outputs":[],"source":["class BasicTokenizer:\n","    def __init__(self, vocab_size):\n","        self.vocab_size = vocab_size\n","        self.vocabulary = {i: bytes([i]) for i in range(256)}\n","        self.merges = {}\n","\n","    def train(self, text, verbose=False):\n","        assert self.vocab_size > 256\n","\n","        ids = list(text.encode(\"utf-8\"))\n","        initial_len = len(ids)\n","\n","        for i in range(self.vocab_size - 256):\n","            pairs = get_pair(ids)\n","            pair = max(pairs, key=pairs.get)\n","            new_id = 256 + i\n","\n","            ids = merge(ids, pair, new_id)\n","            self.merges[pair] = new_id\n","            self.vocabulary[new_id] = (\n","                self.vocabulary[pair[0]] + self.vocabulary[pair[1]]\n","            )\n","\n","        if verbose:\n","            print(\"Compression ratio:\", initial_len / len(ids))"]},{"cell_type":"markdown","id":"14c97bb2-cb54-4982-bbb3-03acc671992e","metadata":{"id":"14c97bb2-cb54-4982-bbb3-03acc671992e"},"source":["## 7. Encoding New Text\n","\n","At inference time, we **must apply merges in the same order they were learned**."]},{"cell_type":"code","execution_count":null,"id":"01b73d17-e6c9-4702-84b8-1dd2c8dfcd16","metadata":{"id":"01b73d17-e6c9-4702-84b8-1dd2c8dfcd16"},"outputs":[],"source":["def encode(self, text):\n","    ids = list(text.encode(\"utf-8\"))\n","\n","    while len(ids) > 1:\n","        pairs = get_pair(ids)\n","        pair = min(pairs, key=lambda p: self.merges.get(p, float(\"inf\")))\n","        if pair not in self.merges:\n","            break\n","        ids = merge(ids, pair, self.merges[pair])\n","\n","    return ids"]},{"cell_type":"markdown","id":"1eba69c7-1821-4ea4-a8b9-a7cfb8fd20d2","metadata":{"id":"1eba69c7-1821-4ea4-a8b9-a7cfb8fd20d2"},"source":["## 8. Decoding Tokens Back to Text\n","\n","Tokenization must be reversible."]},{"cell_type":"code","execution_count":null,"id":"f72afb62-0f5f-4e1f-8c58-77ef6503049b","metadata":{"id":"f72afb62-0f5f-4e1f-8c58-77ef6503049b"},"outputs":[],"source":["def decode(self, ids):\n","        byte_string = b\"\".join(self.vocabulary[i] for i in ids)\n","        return byte_string.decode(\"utf-8\")"]},{"cell_type":"markdown","id":"ce61c8a7-5245-405b-b81b-27b4a72c193d","metadata":{"id":"ce61c8a7-5245-405b-b81b-27b4a72c193d"},"source":["## 9. Let's Practice!"]},{"cell_type":"code","execution_count":null,"id":"991a50e9-2120-4b2c-86b4-76c435aee0bd","metadata":{"id":"991a50e9-2120-4b2c-86b4-76c435aee0bd"},"outputs":[],"source":["class BasicTokenizer:\n","    def __init__(self, vocab_size):\n","        self.vocab_size = vocab_size\n","        self.vocabulary = {i: bytes([i]) for i in range(256)}\n","        self.merges = {}\n","\n","    def train(self, text, verbose=False):\n","        assert self.vocab_size > 256\n","\n","        ids = list(text.encode(\"utf-8\"))\n","        initial_len = len(ids)\n","\n","        for i in range(self.vocab_size - 256):\n","            pairs = get_pair(ids)\n","            pair = max(pairs, key=pairs.get)\n","            new_id = 256 + i\n","\n","            ids = merge(ids, pair, new_id)\n","            self.merges[pair] = new_id\n","            self.vocabulary[new_id] = (\n","                self.vocabulary[pair[0]] + self.vocabulary[pair[1]]\n","            )\n","            #print(sorted( [(v, k) for k,v in pairs.items()], reverse = True) [:10])\n","\n","\n","        if verbose:\n","            print(\"Compression ratio:\", initial_len / len(ids))\n","\n","    def encode(self, text):\n","        ids = list(text.encode(\"utf-8\"))\n","\n","        while len(ids) > 1:\n","            pairs = get_pair(ids)\n","            pair = min(pairs, key=lambda p: self.merges.get(p, float(\"inf\")))\n","            if pair not in self.merges:\n","                break\n","            ids = merge(ids, pair, self.merges[pair])\n","\n","        return ids\n","\n","    def decode(self, ids):\n","        byte_string = b\"\".join(self.vocabulary[i] for i in ids)\n","        return byte_string.decode(\"utf-8\")"]},{"cell_type":"code","execution_count":null,"id":"e14d3bf5-37ef-4d63-b15f-7cb930f11af9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e14d3bf5-37ef-4d63-b15f-7cb930f11af9","executionInfo":{"status":"ok","timestamp":1768197568448,"user_tz":300,"elapsed":4,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"910edf76-d6bb-4a2b-9b37-d8d9927f65a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["The text to encode is the following: hello ğŸ˜„ students\n","Compression ratio: 2.111111111111111\n","[265, 115, 116, 117, 100, 101, 110, 116, 115]\n","hello ğŸ˜„ students\n"]}],"source":["tokenizer = BasicTokenizer(266)\n","print(\"The text to encode is the following:\", text)\n","tokenizer.train(text, verbose=True)\n","\n","encoded = tokenizer.encode(\"hello ğŸ˜„ students\")\n","decoded = tokenizer.decode(encoded)\n","\n","print(encoded)\n","print(decoded)"]},{"cell_type":"code","execution_count":null,"id":"ee801642-77ac-43e3-b676-1b4d88ad18af","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ee801642-77ac-43e3-b676-1b4d88ad18af","executionInfo":{"status":"ok","timestamp":1768197570377,"user_tz":300,"elapsed":4,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"1a44bbd1-b9af-4391-f12f-d1c9d4ee6a43"},"outputs":[{"output_type":"stream","name":"stdout","text":["The text to encode is the following: hello everyone\n","Compression ratio: 3.5\n","[265, 111, 110, 101]\n","hello everyone\n"]}],"source":["tokenizer = BasicTokenizer(266)\n","textTest = \"hello everyone\"\n","print(\"The text to encode is the following:\", textTest)\n","tokenizer.train(textTest, verbose=True)\n","\n","encoded = tokenizer.encode(textTest)\n","decoded = tokenizer.decode(encoded)\n","\n","print(encoded)\n","print(decoded)"]},{"cell_type":"markdown","id":"6b4b4df3-b4dd-4664-b289-047a5d6abe58","metadata":{"id":"6b4b4df3-b4dd-4664-b289-047a5d6abe58"},"source":["## 10. Now , your turn!\n","\n","Try the following:\n","\n","1. Change `vocab_size` and measure compression\n","2. Train on:\n","   - English text\n","   - Code\n","   - Emojis\n","3. Print the first 20 learned merges\n","4. Compare with character-level tokenization\n","\n","âœï¸ Write short answers below each experiment."]},{"cell_type":"code","execution_count":null,"id":"30acfdb6-92f2-41e8-bffc-386fca470397","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"30acfdb6-92f2-41e8-bffc-386fca470397","executionInfo":{"status":"ok","timestamp":1768197588462,"user_tz":300,"elapsed":20,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"03f42fa5-b26b-4af9-d6db-88cd440a64f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["The text to encode is the following: âœï¸ Write short answers below each experiment.\n","Compression ratio: 2.130434782608696\n","[279, 114, 115, 32, 98, 101, 108, 111, 119, 257, 97, 99, 104, 257, 120, 112, 101, 256, 109, 101, 110, 116, 46]\n","âœï¸ Write short answers below each experiment.\n"]}],"source":["tokenizer = BasicTokenizer(280)\n","textTest = \"âœï¸ Write short answers below each experiment.\"\n","print(\"The text to encode is the following:\", textTest)\n","tokenizer.train(textTest, verbose=True)\n","\n","encoded = tokenizer.encode(textTest)\n","decoded = tokenizer.decode(encoded)\n","\n","print(encoded)\n","print(decoded)"]},{"cell_type":"markdown","id":"3951c123-8399-4e6d-a34f-4a1c55e8a843","metadata":{"id":"3951c123-8399-4e6d-a34f-4a1c55e8a843"},"source":["## 10. Why This Matters for Language Models\n","\n","- Tokens â†’ embeddings\n","- Fewer tokens â†’ longer context\n","- Tokenization is the **first inductive bias** of a Transformer\n","- Good tokenization matters more when the model is small.\n","\n","This tokenizer can now be plugged directly into a GPT training loop.\n","\n","<a href=\"https://tiktokenizer.vercel.app/\" >\n","    <img src=\"Tiktokenizer.png\" >\n","</a>"]},{"cell_type":"markdown","id":"4c08948b-4e9e-4ed5-9134-ef1ea162e637","metadata":{"id":"4c08948b-4e9e-4ed5-9134-ef1ea162e637"},"source":["# Regex Tokenizer"]},{"cell_type":"markdown","id":"d3920c2e-7761-4b4c-b970-4dc72b884e10","metadata":{"id":"d3920c2e-7761-4b4c-b970-4dc72b884e10"},"source":["### 1. Why RegexTokenizer Exists\n","\n","Your current basicTokenizer trains BPE on raw byte streams:\n","\n","```\n","text â†’ bytes â†’ BPE merges\n","```\n","\n","That works â€” but it has drawbacks:\n","\n","- BPE may merge across semantic boundaries\n","\n","- Punctuation, whitespace, and numbers get mixed\n","\n","- Training is slower and noisier\n","\n","- Tokens can become syntactically awkward\n","\n","RegexTokenizer fixes this by adding structure before BPE."]},{"cell_type":"markdown","id":"39a4e824-1f80-4dd1-8950-5d5d9d7e7705","metadata":{"id":"39a4e824-1f80-4dd1-8950-5d5d9d7e7705"},"source":["### 2ï¸. Core Idea (One Sentence)\n","\n","RegexTokenizer first splits text into meaningful chunks using regex, then applies BPE inside each chunk independently.\n","\n","This is exactly how GPT-2 / GPT-3 style tokenizers work."]},{"cell_type":"markdown","id":"3786db34-773b-421b-af0d-753eb716dd30","metadata":{"id":"3786db34-773b-421b-af0d-753eb716dd30"},"source":["### 3. High-Level Pipeline\n","\n","```\n","Text\n"," â†“\n","Regex split (words, numbers, punctuation, spaces)\n"," â†“\n","Each chunk â†’ UTF-8 bytes\n"," â†“\n","Byte Pair Encoding (BPE)\n"," â†“\n","Final token IDs\n","```\n","\n","So instead of training BPE on everything, we train it on pre-segmented text units."]},{"cell_type":"markdown","id":"580b734a-2a3f-47dd-9d9a-31c7dd888c86","metadata":{"id":"580b734a-2a3f-47dd-9d9a-31c7dd888c86"},"source":["Letâ€™s decode what it does.\n","\n","What the Regex Captures\n","Pattern\tMeaning\n","\n","| Pattern    | Meaning |\n","| -------- | ------- |\n","| \\p{L}+  | Letters (words)    |\n","| \\p{N}+ | Numbers     |\n","| [^ \\s\\p{L}\\p{N}]+    | Punctuation / symbols    |\n","| \\s+    | Whitespace    |\n","| 's, 't, etc.    | English contractions   |"]},{"cell_type":"code","execution_count":null,"id":"04d5a989-c47f-4510-8d2c-fafb3e78c22c","metadata":{"id":"04d5a989-c47f-4510-8d2c-fafb3e78c22c"},"outputs":[],"source":["import regex as re\n","\n","pattern = re.compile(\n","    r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\", # NOTE: this split pattern deviates from GPT-4 in that it is used \\p{N}{1,2} instead of \\p{N}{1,3}\n","# I did this because I didn't want to \"waste\" too many tokens on numbers for smaller vocab sizes.\n","# I haven't validated that this is actually a good idea, TODO.\n","    re.UNICODE\n",")"]},{"cell_type":"code","execution_count":null,"id":"18946319-155a-432c-a7df-f3ca25364529","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18946319-155a-432c-a7df-f3ca25364529","executionInfo":{"status":"ok","timestamp":1768197597089,"user_tz":300,"elapsed":19,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"b65d998e-b4b9-4dae-af27-a6562cf725b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hello ğŸ˜„ students\n","--------------------------------------------------\n","Splitting Regex Pattern Capture:\n"," ['Hello', ' ğŸ˜„', ' students']\n"]}],"source":["text = \"Hello ğŸ˜„ students\"\n","print(text)\n","print(\"-\"*50)\n","print(\"Splitting Regex Pattern Capture:\\n\", re.findall(pattern, text))"]},{"cell_type":"markdown","id":"803b7041-112d-4265-b51f-d3a852671006","metadata":{"id":"803b7041-112d-4265-b51f-d3a852671006"},"source":["#### ! Important: Spaces are kept, not discarded."]},{"cell_type":"code","execution_count":null,"id":"eccfc9f1-c1e0-4d10-a144-a1978a5378d1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eccfc9f1-c1e0-4d10-a144-a1978a5378d1","executionInfo":{"status":"ok","timestamp":1768197598163,"user_tz":300,"elapsed":11,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"48f6c3f1-30ea-4ba4-f4b2-26e98e71b3e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n","--------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["['ï¼µï½ï½‰ï½ƒï½ï½„ï½…',\n"," '!',\n"," ' ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½',\n"," ' ğŸ‡º\\u200cğŸ‡³\\u200cğŸ‡®\\u200cğŸ‡¨\\u200cğŸ‡´\\u200cğŸ‡©\\u200cğŸ‡ª!',\n"," ' ğŸ˜„',\n"," ' The',\n"," ' very',\n"," ' name',\n"," ' strikes',\n"," ' fear',\n"," ' and',\n"," ' awe',\n"," ' into',\n"," ' the',\n"," ' hearts',\n"," ' of',\n"," ' programmers',\n"," ' worldwide',\n"," '.',\n"," ' We',\n"," ' all',\n"," ' know',\n"," ' we',\n"," ' ought',\n"," ' to',\n"," ' â€œ',\n"," 'support',\n"," ' Unicode',\n"," 'â€',\n"," ' in',\n"," ' our',\n"," ' software',\n"," ' (',\n"," 'whatever',\n"," ' that',\n"," ' means',\n"," 'â€”like',\n"," ' using',\n"," ' wchar',\n"," '_t',\n"," ' for',\n"," ' all',\n"," ' the',\n"," ' strings',\n"," ',',\n"," ' right',\n"," '?).',\n"," ' But',\n"," ' Unicode',\n"," ' can',\n"," ' be',\n"," ' abstruse',\n"," ',',\n"," ' and',\n"," ' diving',\n"," ' into',\n"," ' the',\n"," ' thousand',\n"," '-page',\n"," ' Unicode',\n"," ' Standard',\n"," ' plus',\n"," ' its',\n"," ' dozens',\n"," ' of',\n"," ' supplementary',\n"," ' annexes',\n"," ',',\n"," ' reports',\n"," ',',\n"," ' and',\n"," ' notes',\n"," ' can',\n"," ' be',\n"," ' more',\n"," ' than',\n"," ' a',\n"," ' little',\n"," ' intimidating',\n"," '.',\n"," ' I',\n"," ' don',\n"," 'â€™t',\n"," ' blame',\n"," ' programmers',\n"," ' for',\n"," ' still',\n"," ' finding',\n"," ' the',\n"," ' whole',\n"," ' thing',\n"," ' mysterious',\n"," ',',\n"," ' even',\n"," ' ',\n"," '30',\n"," ' years',\n"," ' after',\n"," ' Unicode',\n"," 'â€™s',\n"," ' inception',\n"," '.']"]},"metadata":{},"execution_count":16}],"source":["text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n","print(text)\n","print(\"-\"*50)\n","re.findall(pattern, text)"]},{"cell_type":"markdown","id":"ec2e76d5-ef98-478f-9c22-4cc08d819698","metadata":{"id":"ec2e76d5-ef98-478f-9c22-4cc08d819698"},"source":["### 3. Why This Matters Before BPE\n","Without Regex (Your Basic Tokenizer)\n","\n","BPE might merge:\n","\n","```\n","\n","\"o \" + \"t\" â†’ \"o t\"\n","\n","```\n","\n","Which is meaningless.\n","\n","With RegexTokenizer\n","\n","BPE operates inside units like:\n","\n","- \"hello\"\n","\n","- \" there\"\n","\n","- \"!\"\n","\n","This ensures:\n","\n","- Cleaner merges\n","\n","- Faster convergence\n","\n","- More interpretable tokens\n"]},{"cell_type":"code","execution_count":null,"id":"20e4a9b4-3de3-43a2-9231-50a41ff9aec1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20e4a9b4-3de3-43a2-9231-50a41ff9aec1","executionInfo":{"status":"ok","timestamp":1768197599305,"user_tz":300,"elapsed":15,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"82a18ab9-3e12-4851-cf13-902057cde309"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," ' there',\n"," '!',\n"," ' I',\n"," \"'m\",\n"," ' ',\n"," '26',\n"," '.',\n"," ' ğŸ˜„\\n',\n"," 'New',\n"," ' line',\n"," '.']"]},"metadata":{},"execution_count":17}],"source":["sample = \"Hello there! I'm 26. ğŸ˜„\\nNew line.\"\n","chunks = re.findall(pattern, sample)\n","chunks"]},{"cell_type":"code","execution_count":null,"id":"478d0af8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"478d0af8","executionInfo":{"status":"ok","timestamp":1768197599606,"user_tz":300,"elapsed":12,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"f6060297-c497-4a3a-ae07-02c2e947b395"},"outputs":[{"output_type":"stream","name":"stdout","text":["b'a' a\n","[240, 159, 152, 132]\n"]}],"source":["vocabulary = {i: bytes([i]) for i in range(256)}\n","print(vocabulary[97], vocabulary[97].decode(\"utf-8\"))  # 'a'\n","print(list(\"ğŸ˜„\".encode(\"utf-8\")))"]},{"cell_type":"markdown","id":"2d4ec971","metadata":{"id":"2d4ec971"},"source":["### BPE primitives: count adjacent pairs + merge\n","\n","BPE repeatedly merges the most frequent adjacent pair into a new token id.\n"]},{"cell_type":"code","execution_count":null,"id":"4c2cea36","metadata":{"id":"4c2cea36"},"outputs":[],"source":["def get_pair(ids, counts):\n","    for pair in zip(ids, ids[1:]):\n","        counts[pair] = counts.get(pair, 0) + 1\n","    return counts\n","\n","def merge(ids, pair, index):\n","    '''\n","    This function will iterate over ids and every time\n","    it sees a instance of pair, it will take that pair\n","    and instead put index , then it will return the list\n","    merge()\n","    list = [1,2, 3, 4, 1, 2]\n","    merge(list, (1,2). 257)\n","    list = [257, 3, 4, 257, 3]\n","    '''\n","\n","    new_ids = []\n","    i = 0\n","    while i < len(ids):\n","        if i <len(ids) - 1 and  (ids[i], ids[i+1]) == pair:\n","            new_ids.append(index)\n","            i += 2\n","        else:\n","            new_ids.append(ids[i])\n","            i += 1\n","    return new_ids"]},{"cell_type":"markdown","id":"d0bb8fcf","metadata":{"id":"d0bb8fcf"},"source":["### 4. RegexTokenizer overview\n","\n","Training:\n","1. Split text into regex chunks\n","2. Convert each chunk to UTF-8 bytes (list of ints)\n","3. Count pairs across **all chunks**\n","4. Merge the most frequent pair across **all chunks**\n","5. Repeat until reaching vocab_size\n","\n","Encoding:\n","1. Split input text into regex chunks\n","2. Encode each chunk with learned merges\n","3. Concatenate token ids\n","\n","Decoding:\n","- Map ids â†’ bytes â†’ UTF-8 string"]},{"cell_type":"code","execution_count":null,"id":"58c120ce","metadata":{"id":"58c120ce"},"outputs":[],"source":["import pickle"]},{"cell_type":"code","execution_count":null,"id":"08d8af5a","metadata":{"id":"08d8af5a"},"outputs":[],"source":["\n","class regexTokenizer:\n","\n","    def __init__(self, vocab_size, pattern):\n","\n","        self.vocab_size = vocab_size\n","        self.vocabulary = {i : bytes([i]) for i in range(256)}\n","        self.merges = {}\n","        self.pattern = re.compile(pattern)\n","\n","    def train(self, text, verbose = False):\n","        # Encode the text\n","        # Iterate over text, self.vocab_size - 256 times\n","        # count all of the pairs in a dictionary\n","        # choose the pair with the highest frequency\n","        # merge that pair as a new token\n","        # add that token to the vocab\n","        # {256: byte_string}\n","        # add to self.merges = {byte_string: 256}\n","\n","        assert self.vocab_size > 256\n","        number_merges = self.vocab_size - 256\n","\n","        text_chunks = re.findall(self.pattern, text)\n","        encoded_chunks = [list(text_chunk.encode('utf-8')) for text_chunk in text_chunks]\n","\n","        length_initial = sum([len(encoded_chunk) for encoded_chunk in encoded_chunks])\n","\n","        for i in range(number_merges):\n","            pairs = {}\n","            for encoded_chunk in encoded_chunks:\n","                pairs = get_pair(encoded_chunk, pairs)\n","\n","            pair = max(pairs, key = pairs.get)\n","            index = 256 + i\n","            encoded_chunks = [merge(encoded_chunk,pair,index) for encoded_chunk in encoded_chunks]\n","            self.merges[pair] = index\n","            self.vocabulary[index] = self.vocabulary[pair[0]] + self.vocabulary[pair[1]]\n","            #print(sorted( [(v, k) for k,v in pairs.items()], reverse = True) [:10])\n","\n","        if verbose:\n","            length_final = sum([len(encoded_chunk) for encoded_chunk in encoded_chunks])\n","            compression = length_initial/length_final\n","            print(length_initial, length_final)\n","            print(compression)\n","\n","    def encode(self, text):\n","\n","        text_chunks = re.findall(self.pattern, text)\n","        encoded_text = []\n","\n","        for text_chunk in text_chunks:\n","            encoded_chunk = self.encode_chunk(text_chunk)\n","            encoded_text.extend(encoded_chunk)\n","        return encoded_text\n","\n","    def encode_chunk(self, text):\n","        '''\n","        self.merges is important here\n","\n","        we get text, and then we convert that text to byte strings, then to integers\n","        and then we iterate over the text until all pairs of\n","        merges that are possible under the trained tokenizer\n","        have been completed\n","\n","        '''\n","\n","        ids = list(text.encode('utf-8'))\n","\n","        for pair, index in self.merges.items():\n","            ids = merge(ids, pair, index)\n","\n","        return ids\n","\n","    def decode(self, ids):\n","        '''\n","        decode gets ids\n","        1. convert the ids to their byte strings\n","        2. convert the byte strings to strings via the vocabulary\n","        3. then return the decoded_text\n","        '''\n","\n","        byte_strings = b''.join([bytes(self.vocabulary[i]) for i in ids])\n","        decoded_text =  byte_strings.decode('utf-8')\n","        return decoded_text\n","\n","    def save(self, path):\n","        with open(path, \"wb\") as file:\n","            pickle.dump(\n","                {\n","                    \"merges\": self.merges,\n","                    \"vocabulary\": self.vocabulary,\n","                    \"pattern\": self.pattern\n","                },\n","                file\n","            )\n","\n","    @classmethod\n","    def load(cls, path):\n","        tokenizer = cls(300, pattern)\n","\n","        with open(path , \"rb\") as file:\n","            data = pickle.load(file)\n","            tokenizer.merges = data[\"merges\"]\n","            tokenizer.vocabulary = data[\"vocabulary\"]\n","            tokenizer.pattern = data[\"pattern\"]\n","        return tokenizer\n"]},{"cell_type":"code","execution_count":null,"id":"389956f7","metadata":{"id":"389956f7"},"outputs":[],"source":["tokenizer = regexTokenizer(300, pattern)"]},{"cell_type":"code","execution_count":null,"id":"411425bd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"411425bd","executionInfo":{"status":"ok","timestamp":1768197603940,"user_tz":300,"elapsed":19,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"53145ca3-a0bf-4bb2-ccd1-ce3584142d6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["616 383\n","1.608355091383812\n"]}],"source":["text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n","tokenizer.train(text, True)"]},{"cell_type":"markdown","id":"f9c02d8f-1ecc-426e-9e52-25383431327f","metadata":{"id":"f9c02d8f-1ecc-426e-9e52-25383431327f"},"source":["# Nanochat Tokenizer\n","\n","We move from educational tokenizers to the **final tokenizer**\n","used in NanoChat.\n","\n","You will learn:\n","- How GPT-style tokenizers handle *chat*\n","- What special tokens are and why they matter\n","- How conversations are rendered into `(input_ids, loss_mask)`\n","- How this enables supervised fine-tuning (SFT)"]},{"cell_type":"code","execution_count":null,"id":"fe6f1bf1-6ebc-4543-b0b0-c0de817e2d56","metadata":{"id":"fe6f1bf1-6ebc-4543-b0b0-c0de817e2d56"},"outputs":[],"source":["# First we need to download the weights https://huggingface.co/karpathy/nanochat-d32/tree/main\n","# Put the tokenizer.pkl in ~/.cache/nanochat/tokenizer directory"]},{"cell_type":"code","source":["!git clone https://github.com/karpathy/nanochat.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXv9NVuMyo9x","executionInfo":{"status":"ok","timestamp":1768197639603,"user_tz":300,"elapsed":835,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"e14ec184-f6c1-481b-c9c0-e247dd5baa90"},"id":"AXv9NVuMyo9x","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'nanochat'...\n","remote: Enumerating objects: 989, done.\u001b[K\n","remote: Counting objects: 100% (154/154), done.\u001b[K\n","remote: Compressing objects: 100% (110/110), done.\u001b[K\n","remote: Total 989 (delta 94), reused 47 (delta 44), pack-reused 835 (from 3)\u001b[K\n","Receiving objects: 100% (989/989), 1.25 MiB | 14.56 MiB/s, done.\n","Resolving deltas: 100% (600/600), done.\n","/content/nanochat\n"]}]},{"cell_type":"code","source":["%cd nanochat"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70KVgkOb0Mg-","executionInfo":{"status":"ok","timestamp":1768199116290,"user_tz":300,"elapsed":48,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"0dea7c75-5fcf-454b-cbf6-840bfe013179"},"id":"70KVgkOb0Mg-","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/nanochat\n"]}]},{"cell_type":"code","source":["# Remove ipykernel and add\n","#[tool.setuptools.packages.find]\n","#where = [\".\"]\n","#include = [\"nanochat*\"]\n","#exclude = [\"class*\", \"dev*\"]\n","# to pyproject.toml\n","!pip install -e ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E5j0LT7H3qy6","executionInfo":{"status":"ok","timestamp":1768199183297,"user_tz":300,"elapsed":13541,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"1b787deb-065c-496f-88a4-8fea26488fb2"},"id":"E5j0LT7H3qy6","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/nanochat\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: datasets>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (4.0.0)\n","Requirement already satisfied: fastapi>=0.117.1 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (0.123.10)\n","Requirement already satisfied: kernels>=0.11.7 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (0.11.7)\n","Requirement already satisfied: matplotlib>=3.10.8 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (3.10.8)\n","Requirement already satisfied: psutil>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (7.2.1)\n","Requirement already satisfied: python-dotenv>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (1.2.1)\n","Requirement already satisfied: regex>=2025.9.1 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (2025.11.3)\n","Requirement already satisfied: rustbpe>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (0.1.0)\n","Requirement already satisfied: scipy>=1.15.3 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (1.16.3)\n","Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (80.9.0)\n","Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (0.9.0)\n","Requirement already satisfied: tiktoken>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (0.12.0)\n","Requirement already satisfied: tokenizers>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (0.22.1)\n","Requirement already satisfied: torch>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (2.9.0+cpu)\n","Requirement already satisfied: transformers>=4.57.3 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (4.57.3)\n","Requirement already satisfied: uvicorn>=0.36.0 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (0.38.0)\n","Requirement already satisfied: wandb>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from nanochat==0.1.0) (0.23.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->nanochat==0.1.0) (3.20.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->nanochat==0.1.0) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->nanochat==0.1.0) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->nanochat==0.1.0) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->nanochat==0.1.0) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->nanochat==0.1.0) (2.32.4)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->nanochat==0.1.0) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->nanochat==0.1.0) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->nanochat==0.1.0) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->nanochat==0.1.0) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->nanochat==0.1.0) (0.36.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->nanochat==0.1.0) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=4.0.0->nanochat==0.1.0) (6.0.3)\n","Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.117.1->nanochat==0.1.0) (0.50.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.117.1->nanochat==0.1.0) (2.12.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.117.1->nanochat==0.1.0) (4.15.0)\n","Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.117.1->nanochat==0.1.0) (0.0.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.8->nanochat==0.1.0) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.8->nanochat==0.1.0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.8->nanochat==0.1.0) (4.61.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.8->nanochat==0.1.0) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.8->nanochat==0.1.0) (11.3.0)\n","Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.8->nanochat==0.1.0) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.8->nanochat==0.1.0) (2.9.0.post0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.9.0->nanochat==0.1.0) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.9.0->nanochat==0.1.0) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.9.0->nanochat==0.1.0) (3.1.6)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.57.3->nanochat==0.1.0) (0.7.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.36.0->nanochat==0.1.0) (8.3.1)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.36.0->nanochat==0.1.0) (0.16.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.21.3->nanochat==0.1.0) (3.1.45)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.21.3->nanochat==0.1.0) (4.5.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.21.3->nanochat==0.1.0) (5.29.5)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.21.3->nanochat==0.1.0) (2.47.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->nanochat==0.1.0) (3.13.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.21.3->nanochat==0.1.0) (4.0.12)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=4.0.0->nanochat==0.1.0) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.117.1->nanochat==0.1.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.117.1->nanochat==0.1.0) (2.41.4)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.117.1->nanochat==0.1.0) (0.4.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.10.8->nanochat==0.1.0) (1.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->nanochat==0.1.0) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->nanochat==0.1.0) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->nanochat==0.1.0) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=4.0.0->nanochat==0.1.0) (2025.11.12)\n","Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi>=0.117.1->nanochat==0.1.0) (4.12.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.9.0->nanochat==0.1.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.9.0->nanochat==0.1.0) (3.0.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->nanochat==0.1.0) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=4.0.0->nanochat==0.1.0) (2025.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->nanochat==0.1.0) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->nanochat==0.1.0) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->nanochat==0.1.0) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->nanochat==0.1.0) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->nanochat==0.1.0) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->nanochat==0.1.0) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0.0->nanochat==0.1.0) (1.22.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.21.3->nanochat==0.1.0) (5.0.2)\n","Building wheels for collected packages: nanochat\n","  Building editable for nanochat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nanochat: filename=nanochat-0.1.0-0.editable-py3-none-any.whl size=10029 sha256=07fea20e7851d151abdb06f09e0a7046ec22993d4ac6a3982867cbed9c91ac8a\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2vvx32o4/wheels/59/44/68/4f0e259f1e3efb353b7dc9ec0502623edda1ea438a24e9f48f\n","Successfully built nanochat\n","Installing collected packages: nanochat\n","  Attempting uninstall: nanochat\n","    Found existing installation: nanochat 0.1.0\n","    Uninstalling nanochat-0.1.0:\n","      Successfully uninstalled nanochat-0.1.0\n","Successfully installed nanochat-0.1.0\n"]}]},{"cell_type":"code","execution_count":4,"id":"326ed9c5-b1e9-4516-8b7c-731402cb49c8","metadata":{"id":"326ed9c5-b1e9-4516-8b7c-731402cb49c8","executionInfo":{"status":"ok","timestamp":1768199194818,"user_tz":300,"elapsed":2,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["from nanochat.tokenizer import get_tokenizer"]},{"cell_type":"code","source":["%cd /root"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_F3jHrY4zQO","executionInfo":{"status":"ok","timestamp":1768199249704,"user_tz":300,"elapsed":25,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"4a247bff-5957-4d5c-e857-b8cc66a95447"},"id":"Z_F3jHrY4zQO","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n"]}]},{"cell_type":"code","source":["%mkdir /root/.cache/nanochat/tokenizer/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VBnCm1np4122","executionInfo":{"status":"ok","timestamp":1768199310212,"user_tz":300,"elapsed":109,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"b5ef50f6-36e2-4136-c84c-b5ad3167177c"},"id":"VBnCm1np4122","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory â€˜/root/.cache/nanochat/tokenizer/â€™: File exists\n"]}]},{"cell_type":"code","source":["%cd /root/.cache/nanochat/tokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kn8gd5tf4-PT","executionInfo":{"status":"ok","timestamp":1768199331826,"user_tz":300,"elapsed":15,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"d56546ce-b672-411f-cd5d-67a5e4287565"},"id":"kn8gd5tf4-PT","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["/root/.cache/nanochat/tokenizer\n"]}]},{"cell_type":"code","source":["from huggingface_hub import hf_hub_download\n","# Download the specific file and get its local file path\n","tokenizer_pkl_path = hf_hub_download(repo_id=\"karpathy/nanochat-d34\", filename=\"tokenizer.pkl\")\n","tokenizer_pt_path = hf_hub_download(repo_id=\"karpathy/nanochat-d34\", filename=\"token_bytes.pt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211,"referenced_widgets":["b9e5f7552da241cbbe837f794f2c0d4e","438b016ff9474734abc6722bd1fa476c","477b5c03e1aa475bbf3197777ee50cb1","f5bec8cfcfde40d5ab8a50709b7939e4","68f66bb2c0484b2bbde922aad4063334","6610ee9b181a4342a98d42acd542588c","70278ac698924c95b23ecab89d7f0c9c","093d0bc780914c9fbd8694b1243d2704","9f21c0d484f7490f85d6d2ec47d28c1b","1bfed3f7b77e4771a4b8c9cdc9bb17e2","0aaff05ce72247aa8f2afd4cd1c79419","b0a68cd925764c6cbf5ffeecad4edf42","d22f2953601a4e1a9583e8f8fd05efdf","5518f41c1e1e4357ba02ae4672c96f59","e73a4fd65d514e529d4859f6fc79efdf","9a4914cd8a96467ca9c66a28d0c0ed39","4548a069987d44ad9659a43cc7aa2cd4","8a7b6ddc460a46f0abd442ba5ef50194","171818255ba244cb99129caae14ffe39","b4332d1eab3648f187082f9ff1d555b7","b170638e35584804bf8be9da9cf1f155","b6308099df2240889221fef4af2ca133"]},"id":"RVxEXyRx5Kmg","executionInfo":{"status":"ok","timestamp":1768199385916,"user_tz":300,"elapsed":4818,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"00f8aa93-4c18-44be-ca3e-7023b9069e6d"},"id":"RVxEXyRx5Kmg","execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer.pkl:   0%|          | 0.00/846k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9e5f7552da241cbbe837f794f2c0d4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["token_bytes.pt:   0%|          | 0.00/264k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0a68cd925764c6cbf5ffeecad4edf42"}},"metadata":{}}]},{"cell_type":"code","source":["tokenizer_pkl_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"NA7eTPA-5bM9","executionInfo":{"status":"ok","timestamp":1768199403856,"user_tz":300,"elapsed":36,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"061846bd-b898-4b4e-eea7-3f2247475792"},"id":"NA7eTPA-5bM9","execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/root/.cache/huggingface/hub/models--karpathy--nanochat-d34/snapshots/c48357d43863a3a6cdc5f5db5b4ec5964e4192d6/tokenizer.pkl'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["%cp '/root/.cache/huggingface/hub/models--karpathy--nanochat-d34/snapshots/c48357d43863a3a6cdc5f5db5b4ec5964e4192d6/tokenizer.pkl' '/root/.cache/nanochat/tokenizer/tokenizer.pkl'"],"metadata":{"id":"9mr35FsV5Xwh","executionInfo":{"status":"ok","timestamp":1768199452858,"user_tz":300,"elapsed":112,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}}},"id":"9mr35FsV5Xwh","execution_count":18,"outputs":[]},{"cell_type":"code","source":["%cd /content/nanochat/\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXE5XiTn5qAm","executionInfo":{"status":"ok","timestamp":1768199493877,"user_tz":300,"elapsed":172,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"26541629-2c1b-40b0-cc70-5e3786ca3912"},"id":"OXE5XiTn5qAm","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/nanochat\n","\u001b[0m\u001b[01;34mdev\u001b[0m/           \u001b[01;34mnanochat\u001b[0m/           README.md        \u001b[01;34mscripts\u001b[0m/     \u001b[01;34mtests\u001b[0m/\n","LICENSE        \u001b[01;34mnanochat.egg-info\u001b[0m/  run1000.sh       speedrun.sh  uv.lock\n","miniseries.sh  pyproject.toml      scaling_laws.sh  \u001b[01;34mtasks\u001b[0m/\n"]}]},{"cell_type":"code","execution_count":19,"id":"303984c8-0355-480e-b9f1-4c7df14560bc","metadata":{"id":"303984c8-0355-480e-b9f1-4c7df14560bc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768199456497,"user_tz":300,"elapsed":143,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"7c137f09-e916-4f1b-f438-3f4864cef31b"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'nanochat.tokenizer.RustBPETokenizer'>\n"]}],"source":["tokenizer = get_tokenizer()\n","print(type(tokenizer))"]},{"cell_type":"markdown","id":"4c633459-e121-436c-976b-65f52c90d811","metadata":{"id":"4c633459-e121-436c-976b-65f52c90d811"},"source":["### 1. Special Tokens\n","\n","Chat models need *control tokens* to:\n","- separate user vs assistant\n","- delimit messages\n","- support tools (python blocks, outputs)"]},{"cell_type":"code","execution_count":21,"id":"b5b607d4-0429-4fbc-88fa-7fd814af1f67","metadata":{"id":"b5b607d4-0429-4fbc-88fa-7fd814af1f67","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768199499061,"user_tz":300,"elapsed":19,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"237836b8-d836-4314-cca7-bd1b6513393b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'<|assistant_end|>',\n"," '<|assistant_start|>',\n"," '<|bos|>',\n"," '<|output_end|>',\n"," '<|output_start|>',\n"," '<|python_end|>',\n"," '<|python_start|>',\n"," '<|user_end|>',\n"," '<|user_start|>'}"]},"metadata":{},"execution_count":21}],"source":["special_tokens = tokenizer.get_special_tokens()\n","special_tokens"]},{"cell_type":"code","execution_count":22,"id":"84dec289-dd9b-40d8-bfe5-15e6e7ddb626","metadata":{"id":"84dec289-dd9b-40d8-bfe5-15e6e7ddb626","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768199500233,"user_tz":300,"elapsed":18,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"9699ea71-0243-40d8-dee1-87b2715980ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["<|output_start|>          â†’ id 65534\n","<|assistant_end|>         â†’ id 65531\n","<|python_start|>          â†’ id 65532\n","<|python_end|>            â†’ id 65533\n","<|user_end|>              â†’ id 65529\n","<|bos|>                   â†’ id 65527\n","<|assistant_start|>       â†’ id 65530\n","<|user_start|>            â†’ id 65528\n","<|output_end|>            â†’ id 65535\n"]}],"source":["# Lets encode these special tokens\n","for tok in special_tokens:\n","    print(f\"{tok:25s} â†’ id {tokenizer.encode_special(tok)}\")"]},{"cell_type":"code","execution_count":23,"id":"731db13e-c0f9-453d-a9b9-b0c414e35722","metadata":{"id":"731db13e-c0f9-453d-a9b9-b0c414e35722","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768199501304,"user_tz":300,"elapsed":26,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"770093d0-b078-4e1f-95c5-97c05853df2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["65534 â†’ id <|output_start|>\n","65531 â†’ id <|assistant_end|>\n","65532 â†’ id <|python_start|>\n","65533 â†’ id <|python_end|>\n","65529 â†’ id <|user_end|>\n","65527 â†’ id <|bos|>\n","65530 â†’ id <|assistant_start|>\n","65528 â†’ id <|user_start|>\n","65535 â†’ id <|output_end|>\n"]}],"source":["# Lets decode it again to see if everything works well\n","encoded_special_tokens = [ tokenizer.encode_special(tok) for tok in list(special_tokens) ]\n","for id in encoded_special_tokens:\n","    print(f\"{id} â†’ id {tokenizer.decode([id])}\")"]},{"cell_type":"code","execution_count":24,"id":"0b06c865-c9e9-4fc5-a01e-c3a77e0b734b","metadata":{"id":"0b06c865-c9e9-4fc5-a01e-c3a77e0b734b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768199502035,"user_tz":300,"elapsed":35,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"68dd1018-8f7e-4963-8d2f-b5f6ba45a2ad"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["([12167,\n","  181,\n","  239,\n","  189,\n","  142,\n","  239,\n","  189,\n","  137,\n","  239,\n","  189,\n","  131,\n","  239,\n","  189,\n","  143,\n","  239,\n","  189,\n","  132,\n","  239,\n","  189,\n","  133,\n","  33,\n","  20524,\n","  133,\n","  164,\n","  14899,\n","  133,\n","  157,\n","  14899,\n","  133,\n","  152,\n","  14899,\n","  133,\n","  146,\n","  14899,\n","  133,\n","  158,\n","  14899,\n","  133,\n","  147,\n","  14899,\n","  133,\n","  148,\n","  308,\n","  189,\n","  20524,\n","  135,\n","  186,\n","  308,\n","  140,\n","  50936,\n","  179,\n","  308,\n","  140,\n","  50936,\n","  174,\n","  308,\n","  140,\n","  50936,\n","  168,\n","  308,\n","  140,\n","  50936,\n","  180,\n","  308,\n","  140,\n","  50936,\n","  169,\n","  308,\n","  140,\n","  50936,\n","  170,\n","  33,\n","  46824,\n","  132,\n","  361,\n","  907,\n","  1588,\n","  13591,\n","  3615,\n","  288,\n","  24500,\n","  636,\n","  261,\n","  12164,\n","  281,\n","  20942,\n","  5425,\n","  46,\n","  1006,\n","  500,\n","  675,\n","  384,\n","  11814,\n","  287,\n","  549,\n","  46955,\n","  38226,\n","  507,\n","  283,\n","  659,\n","  3076,\n","  372,\n","  56571,\n","  332,\n","  1452,\n","  36656,\n","  1034,\n","  270,\n","  4210,\n","  31930,\n","  327,\n","  500,\n","  261,\n","  12736,\n","  44,\n","  1037,\n","  42544,\n","  1208,\n","  38226,\n","  400,\n","  311,\n","  445,\n","  9129,\n","  312,\n","  44,\n","  288,\n","  17719,\n","  636,\n","  261,\n","  6557,\n","  18645,\n","  38226,\n","  9306,\n","  7173,\n","  643,\n","  12953,\n","  281,\n","  28274,\n","  19597,\n","  269,\n","  44,\n","  3988,\n","  44,\n","  288,\n","  4238,\n","  400,\n","  311,\n","  498,\n","  617,\n","  257,\n","  1658,\n","  31163,\n","  46,\n","  338,\n","  1294,\n","  846,\n","  11988,\n","  20942,\n","  327,\n","  1253,\n","  4373,\n","  261,\n","  2246,\n","  2558,\n","  13617,\n","  44,\n","  906,\n","  32,\n","  1286,\n","  811,\n","  917,\n","  38226,\n","  443,\n","  24493,\n","  46],\n"," 'ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡º\\u200cğŸ‡³\\u200cğŸ‡®\\u200cğŸ‡¨\\u200cğŸ‡´\\u200cğŸ‡©\\u200cğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.')"]},"metadata":{},"execution_count":24}],"source":["# Lets try with the text we already worked\n","text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n","ids = tokenizer.encode(text)\n","decoded = tokenizer.decode(ids)\n","\n","ids, decoded"]},{"cell_type":"code","execution_count":25,"id":"e1d532d9-71cb-40aa-bad8-0e8cb7ec85aa","metadata":{"id":"e1d532d9-71cb-40aa-bad8-0e8cb7ec85aa","executionInfo":{"status":"ok","timestamp":1768199505322,"user_tz":300,"elapsed":39,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["conversation = {\n","    \"messages\": [\n","        {\"role\": \"user\", \"content\": \"What is a transformer?\"},\n","        {\"role\": \"assistant\", \"content\": \"A transformer is a neural network based on attention.\"}\n","    ]\n","}"]},{"cell_type":"code","execution_count":26,"id":"d9c04aad-df97-411d-bad4-f9ffe0f9aeb9","metadata":{"id":"d9c04aad-df97-411d-bad4-f9ffe0f9aeb9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768199505916,"user_tz":300,"elapsed":19,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"f91b92c8-b2e6-4d42-83e7-088dbb9af2f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of tokens: 20\n","Loss tokens: 11\n"]}],"source":["ids, loss_mask = tokenizer.render_conversation(conversation)\n","\n","print(\"Number of tokens:\", len(ids))\n","print(\"Loss tokens:\", sum(loss_mask))"]},{"cell_type":"code","execution_count":27,"id":"6e5b1be7-ea68-402b-9073-f8f18f877b94","metadata":{"id":"6e5b1be7-ea68-402b-9073-f8f18f877b94","executionInfo":{"status":"ok","timestamp":1768199507071,"user_tz":300,"elapsed":13,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["def printUserAssistantType(mask):\n","    if mask == 0:\n","        return \"User\"\n","    else:\n","        return \"Assistant\""]},{"cell_type":"code","execution_count":28,"id":"5984f92d-c15d-4e73-aeea-34bf4a55b323","metadata":{"id":"5984f92d-c15d-4e73-aeea-34bf4a55b323","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768199507751,"user_tz":300,"elapsed":19,"user":{"displayName":"RENATO AARÃ“N CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"65909321-b43d-4639-b795-8a5f8474209a"},"outputs":[{"output_type":"stream","name":"stdout","text":["'<|bos|>'             mask=0 User\n","'<|user_start|>'      mask=0 User\n","'What'                mask=0 User\n","' is'                 mask=0 User\n","' a'                  mask=0 User\n","' transformer'        mask=0 User\n","'?'                   mask=0 User\n","'<|user_end|>'        mask=0 User\n","'<|assistant_start|>'  mask=0 User\n","'A'                   mask=1 Assistant\n","' transformer'        mask=1 Assistant\n","' is'                 mask=1 Assistant\n","' a'                  mask=1 Assistant\n","' neural'             mask=1 Assistant\n","' network'            mask=1 Assistant\n","' based'              mask=1 Assistant\n","' on'                 mask=1 Assistant\n","' attention'          mask=1 Assistant\n","'.'                   mask=1 Assistant\n","'<|assistant_end|>'   mask=1 Assistant\n"]}],"source":["decoded_tokens = [tokenizer.decode([i]) for i in ids]\n","\n","for t, m in zip(decoded_tokens, loss_mask):\n","    print(f\"{repr(t):20s}  mask={m} {printUserAssistantType(m)}\")"]},{"cell_type":"markdown","id":"c37684ee-9f41-4a85-a850-fa135c8629f2","metadata":{"id":"c37684ee-9f41-4a85-a850-fa135c8629f2"},"source":["### 2. Why Masking?\n","\n","Without masking:\n","- model would be trained to predict user prompts\n","- learning becomes unstable\n","- chat behavior degrades\n","\n","Masking enforces:\n","P(assistant | user)"]},{"cell_type":"markdown","id":"55d53edf-210f-4cd6-8b17-4c970e5d5403","metadata":{"id":"55d53edf-210f-4cd6-8b17-4c970e5d5403"},"source":["### 3. Exercise (15 minutes)\n","1. Create a 3-turn conversation\n","2. Render it\n","3. Count how many tokens are supervised\n","4. Inspect where assistant supervision starts"]},{"cell_type":"code","execution_count":null,"id":"91b6a4fe-dd46-407a-b357-98837611c4e7","metadata":{"id":"91b6a4fe-dd46-407a-b357-98837611c4e7"},"outputs":[],"source":["## TO DO"]},{"cell_type":"markdown","id":"09aad174-9433-4fa5-ad76-7fd1e03afa5c","metadata":{"id":"09aad174-9433-4fa5-ad76-7fd1e03afa5c"},"source":["## Next Step\n","\n","We now have:\n","- Token IDs\n","- Loss masks\n","\n","Next:\n","â¡ Let's understand GPT"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"panel-cell-order":["f01e1d45-3aab-465c-9480-fc7dc5106e69","306cd8b2-2a76-422a-b78c-f181e1a64620","bdae7223-9d36-4c45-93a2-bf4fd6a74bcd"],"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"b9e5f7552da241cbbe837f794f2c0d4e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_438b016ff9474734abc6722bd1fa476c","IPY_MODEL_477b5c03e1aa475bbf3197777ee50cb1","IPY_MODEL_f5bec8cfcfde40d5ab8a50709b7939e4"],"layout":"IPY_MODEL_68f66bb2c0484b2bbde922aad4063334"}},"438b016ff9474734abc6722bd1fa476c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6610ee9b181a4342a98d42acd542588c","placeholder":"â€‹","style":"IPY_MODEL_70278ac698924c95b23ecab89d7f0c9c","value":"tokenizer.pkl:â€‡100%"}},"477b5c03e1aa475bbf3197777ee50cb1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_093d0bc780914c9fbd8694b1243d2704","max":846092,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f21c0d484f7490f85d6d2ec47d28c1b","value":846092}},"f5bec8cfcfde40d5ab8a50709b7939e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bfed3f7b77e4771a4b8c9cdc9bb17e2","placeholder":"â€‹","style":"IPY_MODEL_0aaff05ce72247aa8f2afd4cd1c79419","value":"â€‡846k/846kâ€‡[00:02&lt;00:00,â€‡417kB/s]"}},"68f66bb2c0484b2bbde922aad4063334":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6610ee9b181a4342a98d42acd542588c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70278ac698924c95b23ecab89d7f0c9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"093d0bc780914c9fbd8694b1243d2704":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f21c0d484f7490f85d6d2ec47d28c1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1bfed3f7b77e4771a4b8c9cdc9bb17e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0aaff05ce72247aa8f2afd4cd1c79419":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0a68cd925764c6cbf5ffeecad4edf42":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d22f2953601a4e1a9583e8f8fd05efdf","IPY_MODEL_5518f41c1e1e4357ba02ae4672c96f59","IPY_MODEL_e73a4fd65d514e529d4859f6fc79efdf"],"layout":"IPY_MODEL_9a4914cd8a96467ca9c66a28d0c0ed39"}},"d22f2953601a4e1a9583e8f8fd05efdf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4548a069987d44ad9659a43cc7aa2cd4","placeholder":"â€‹","style":"IPY_MODEL_8a7b6ddc460a46f0abd442ba5ef50194","value":"token_bytes.pt:â€‡100%"}},"5518f41c1e1e4357ba02ae4672c96f59":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_171818255ba244cb99129caae14ffe39","max":263721,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4332d1eab3648f187082f9ff1d555b7","value":263721}},"e73a4fd65d514e529d4859f6fc79efdf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b170638e35584804bf8be9da9cf1f155","placeholder":"â€‹","style":"IPY_MODEL_b6308099df2240889221fef4af2ca133","value":"â€‡264k/264kâ€‡[00:01&lt;00:00,â€‡154kB/s]"}},"9a4914cd8a96467ca9c66a28d0c0ed39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4548a069987d44ad9659a43cc7aa2cd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a7b6ddc460a46f0abd442ba5ef50194":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"171818255ba244cb99129caae14ffe39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4332d1eab3648f187082f9ff1d555b7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b170638e35584804bf8be9da9cf1f155":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6308099df2240889221fef4af2ca133":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}