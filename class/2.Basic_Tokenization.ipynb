{"cells":[{"cell_type":"markdown","id":"f01e1d45-3aab-465c-9480-fc7dc5106e69","metadata":{"panel-layout":{"height":60.59375,"visible":true,"width":100},"id":"f01e1d45-3aab-465c-9480-fc7dc5106e69"},"source":["# Small Language Workshop Part 1"]},{"cell_type":"markdown","id":"306cd8b2-2a76-422a-b78c-f181e1a64620","metadata":{"panel-layout":{"height":220.421875,"visible":true,"width":100},"id":"306cd8b2-2a76-422a-b78c-f181e1a64620"},"source":["# Tokenization from Scratch: Building a Byte Pair Encoder (BPE)\n","\n","In this notebook, we will implement a **byte-level tokenizer** similar to the one used in GPT-style models.\n","\n","By the end, you will:\n","- Understand why tokenization is necessary\n","- Implement Byte Pair Encoding from scratch\n","- Train a tokenizer on real Unicode text\n","- Encode and decode text losslessly\n","- Understand why tokenization matters for Small Language Models"]},{"cell_type":"markdown","id":"bdae7223-9d36-4c45-93a2-bf4fd6a74bcd","metadata":{"panel-layout":{"height":223.234375,"visible":true,"width":100},"id":"bdae7223-9d36-4c45-93a2-bf4fd6a74bcd"},"source":["## 1. Why Tokenization?\n","\n","Neural networks operate on **numbers**, but language is **text**.\n","\n","Key challenges:\n","- Variable-length sequences\n","- Unicode & emojis\n","- Finite vocabulary\n","- Efficiency (context length matters!)\n","\n","üí° **Key idea:** Tokenization is a *compression problem*."]},{"cell_type":"markdown","id":"7aa9107a-b677-4b2a-aab2-a18757cce42b","metadata":{"id":"7aa9107a-b677-4b2a-aab2-a18757cce42b"},"source":["<img src='Neural_Network.png'>"]},{"cell_type":"markdown","id":"b64c1823-9a51-408f-81b5-3adf864b17d8","metadata":{"id":"b64c1823-9a51-408f-81b5-3adf864b17d8"},"source":["## 2. Text ‚Üí Bytes\n","\n","Instead of characters or words, GPT-style models operate on **bytes**.\n","\n","Why?\n","- Every string can be represented as bytes\n","- No unknown tokens\n","- Unicode-safe"]},{"cell_type":"code","execution_count":null,"id":"09e5f92a-3350-4be8-b11d-6162efcfda79","metadata":{"id":"09e5f92a-3350-4be8-b11d-6162efcfda79","executionInfo":{"status":"aborted","timestamp":1768211226073,"user_tz":300,"elapsed":227,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["text = \"hello üòÑ students\"\n","encoded = text.encode(\"utf-8\")\n","\n","print(\"Original text:\", text)\n","print(\"UTF-8 Encoding:\", encoded) # Bytes values\n","print(\"List Encoded UTF-8:\", list(encoded)) # Decimal Values in Hexadecimal"]},{"cell_type":"markdown","id":"412d4f8c-61d0-44c4-8b7d-8ffbda3ae31d","metadata":{"id":"412d4f8c-61d0-44c4-8b7d-8ffbda3ae31d"},"source":["üß† **Think**\n","- Why does the emoji produce multiple numbers?\n","- Why might this be better than character-level tokenization?"]},{"cell_type":"markdown","id":"7ee7b3d8-be11-48b0-8327-87e2e91fecf8","metadata":{"id":"7ee7b3d8-be11-48b0-8327-87e2e91fecf8"},"source":["## 3. Initial Vocabulary\n","\n","We start with a vocabulary of **256 tokens**, one for each possible byte."]},{"cell_type":"code","execution_count":2,"id":"6b970630-c84a-4f9c-887e-83512313fe3b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"6b970630-c84a-4f9c-887e-83512313fe3b","executionInfo":{"status":"error","timestamp":1768211226109,"user_tz":300,"elapsed":27,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}},"outputId":"b82211f0-0e3b-4ec4-e971-36ebf1a25c76"},"outputs":[{"output_type":"stream","name":"stdout","text":["b'a' a\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'encoded' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1856157720.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# So we can take the previous values of the string \"hello üòÑ students\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m104\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# It should match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# It should match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'encoded' is not defined"]}],"source":["vocab = {i: bytes([i]) for i in range(256)}\n","\n","# Sanity check\n","print(vocab[97], vocab[97].decode(\"utf-8\"))  # 'a'\n","\n","# So we can take the previous values of the string \"hello üòÑ students\"\n","print(vocab[104], bytes([encoded[0]]))  # It should match\n","print(vocab[240], bytes([encoded[6]]))  # It should match"]},{"cell_type":"markdown","id":"74347d31-bfd2-414e-890e-ff13222be5dc","metadata":{"id":"74347d31-bfd2-414e-890e-ff13222be5dc"},"source":["## 4. Counting Adjacent Pairs\n","\n","Byte Pair Encoding works by repeatedly merging the **most frequent adjacent pair**."]},{"cell_type":"code","execution_count":null,"id":"0560b4d0-8fd9-4167-84ce-6c4dc2b62570","metadata":{"id":"0560b4d0-8fd9-4167-84ce-6c4dc2b62570","executionInfo":{"status":"aborted","timestamp":1768211226202,"user_tz":300,"elapsed":2,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["def get_pair(ids):\n","    counts = {}\n","    for pair in zip(ids, ids[1:]):\n","        # TODO: count how many times each pair appears\n","\n","    return counts"]},{"cell_type":"code","execution_count":null,"id":"c72a792a-e430-497b-ba57-54495569e0a3","metadata":{"id":"c72a792a-e430-497b-ba57-54495569e0a3","executionInfo":{"status":"aborted","timestamp":1768211226225,"user_tz":300,"elapsed":372,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["ids = [1, 2, 3, 1, 2]\n","print(get_pair(ids))\n","\n","# (1,1) or (2,2) does not count because the elements are equal"]},{"cell_type":"markdown","id":"b3c97e5e-83bf-47d9-9240-e70ae279d7a5","metadata":{"id":"b3c97e5e-83bf-47d9-9240-e70ae279d7a5"},"source":["## 5. Merging Pairs\n","\n","When we merge a pair `(a, b)` into a new token `k`,\n","we replace all occurrences of `(a, b)` with `k`."]},{"cell_type":"markdown","id":"329cc33f-251c-4b8a-b180-6ecd6f90a87f","metadata":{"id":"329cc33f-251c-4b8a-b180-6ecd6f90a87f"},"source":["<img src = 'BPE_Algorithm.png'>"]},{"cell_type":"code","execution_count":null,"id":"62ee8903-1912-45f5-8749-ec4c084fb5f0","metadata":{"id":"62ee8903-1912-45f5-8749-ec4c084fb5f0","executionInfo":{"status":"aborted","timestamp":1768211226250,"user_tz":300,"elapsed":4,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["def merge(ids, pair, index):\n","    new_ids = []\n","    i = 0\n","    while i < len(ids):\n","        if i < len(ids) - 1 and (ids[i], ids[i+1]) == pair:\n","            new_ids.append(index)\n","            i += 2\n","        else:\n","            new_ids.append(ids[i])\n","            i += 1\n","    return new_ids"]},{"cell_type":"code","execution_count":null,"id":"ed48a2a6-fe23-4fe8-a16c-87303ab69ba9","metadata":{"id":"ed48a2a6-fe23-4fe8-a16c-87303ab69ba9","executionInfo":{"status":"aborted","timestamp":1768211226252,"user_tz":300,"elapsed":5,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["merge([1, 2, 3, 1, 2], (1, 2), 256)"]},{"cell_type":"markdown","id":"124acf4b-018c-4d36-a3dc-6c87fe002542","metadata":{"id":"124acf4b-018c-4d36-a3dc-6c87fe002542"},"source":["What happened to sequence length? Why is this useful?"]},{"cell_type":"markdown","id":"162fa746-9ecf-4a1b-b560-acb1f9ad42f6","metadata":{"id":"162fa746-9ecf-4a1b-b560-acb1f9ad42f6"},"source":["## 6. Training the Tokenizer\n","\n","We will now repeatedly:\n","1. Count all adjacent pairs\n","2. Select the most frequent one\n","3. Merge it into a new token"]},{"cell_type":"code","execution_count":null,"id":"2bc14afb-3418-4758-8987-ec6e3bf69c97","metadata":{"id":"2bc14afb-3418-4758-8987-ec6e3bf69c97","executionInfo":{"status":"aborted","timestamp":1768211226253,"user_tz":300,"elapsed":5,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["class BasicTokenizer:\n","    def __init__(self, vocab_size):\n","        self.vocab_size = vocab_size\n","        self.vocabulary = {i: bytes([i]) for i in range(256)}\n","        self.merges = {}\n","\n","    def train(self, text, verbose=False):\n","        assert self.vocab_size > 256\n","\n","        ids = list(text.encode(\"utf-8\"))\n","        initial_len = len(ids)\n","\n","        for i in range(self.vocab_size - 256):\n","            pairs = get_pair(ids)\n","            pair = max(pairs, key=pairs.get)\n","            new_id = 256 + i\n","\n","            ids = merge(ids, pair, new_id)\n","            self.merges[pair] = new_id\n","            self.vocabulary[new_id] = (\n","                self.vocabulary[pair[0]] + self.vocabulary[pair[1]]\n","            )\n","\n","        if verbose:\n","            print(\"Compression ratio:\", initial_len / len(ids))"]},{"cell_type":"markdown","id":"14c97bb2-cb54-4982-bbb3-03acc671992e","metadata":{"id":"14c97bb2-cb54-4982-bbb3-03acc671992e"},"source":["## 7. Encoding New Text\n","\n","At inference time, we **must apply merges in the same order they were learned**."]},{"cell_type":"code","execution_count":null,"id":"01b73d17-e6c9-4702-84b8-1dd2c8dfcd16","metadata":{"id":"01b73d17-e6c9-4702-84b8-1dd2c8dfcd16","executionInfo":{"status":"aborted","timestamp":1768211226254,"user_tz":300,"elapsed":5,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["def encode(self, text):\n","    ids = list(text.encode(\"utf-8\"))\n","\n","    while len(ids) > 1:\n","        pairs = get_pair(ids)\n","        pair = min(pairs, key=lambda p: self.merges.get(p, float(\"inf\")))\n","        if pair not in self.merges:\n","            break\n","        ids = merge(ids, pair, self.merges[pair])\n","\n","    return ids"]},{"cell_type":"markdown","id":"1eba69c7-1821-4ea4-a8b9-a7cfb8fd20d2","metadata":{"id":"1eba69c7-1821-4ea4-a8b9-a7cfb8fd20d2"},"source":["## 8. Decoding Tokens Back to Text\n","\n","Tokenization must be reversible."]},{"cell_type":"code","execution_count":null,"id":"f72afb62-0f5f-4e1f-8c58-77ef6503049b","metadata":{"id":"f72afb62-0f5f-4e1f-8c58-77ef6503049b","executionInfo":{"status":"aborted","timestamp":1768211226254,"user_tz":300,"elapsed":4,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["def decode(self, ids):\n","        byte_string = b\"\".join(self.vocabulary[i] for i in ids)\n","        return byte_string.decode(\"utf-8\")"]},{"cell_type":"markdown","id":"ce61c8a7-5245-405b-b81b-27b4a72c193d","metadata":{"id":"ce61c8a7-5245-405b-b81b-27b4a72c193d"},"source":["## 9. Let's Practice!"]},{"cell_type":"code","execution_count":null,"id":"991a50e9-2120-4b2c-86b4-76c435aee0bd","metadata":{"id":"991a50e9-2120-4b2c-86b4-76c435aee0bd","executionInfo":{"status":"aborted","timestamp":1768211226255,"user_tz":300,"elapsed":398,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["class BasicTokenizer:\n","    def __init__(self, vocab_size):\n","        self.vocab_size = vocab_size\n","        self.vocabulary = {i: bytes([i]) for i in range(256)}\n","        self.merges = {}\n","\n","    def train(self, text, verbose=False):\n","        assert self.vocab_size > 256\n","\n","        ids = list(text.encode(\"utf-8\"))\n","        initial_len = len(ids)\n","\n","        for i in range(self.vocab_size - 256):\n","            pairs = get_pair(ids)\n","            pair = max(pairs, key=pairs.get)\n","            new_id = 256 + i\n","\n","            ids = merge(ids, pair, new_id)\n","            self.merges[pair] = new_id\n","            self.vocabulary[new_id] = (\n","                self.vocabulary[pair[0]] + self.vocabulary[pair[1]]\n","            )\n","            #print(sorted( [(v, k) for k,v in pairs.items()], reverse = True) [:10])\n","\n","\n","        if verbose:\n","            print(\"Compression ratio:\", initial_len / len(ids))\n","\n","    def encode(self, text):\n","        ids = list(text.encode(\"utf-8\"))\n","\n","        while len(ids) > 1:\n","            pairs = get_pair(ids)\n","            pair = min(pairs, key=lambda p: self.merges.get(p, float(\"inf\")))\n","            if pair not in self.merges:\n","                break\n","            ids = merge(ids, pair, self.merges[pair])\n","\n","        return ids\n","\n","    def decode(self, ids):\n","        byte_string = b\"\".join(self.vocabulary[i] for i in ids)\n","        return byte_string.decode(\"utf-8\")"]},{"cell_type":"code","execution_count":null,"id":"e14d3bf5-37ef-4d63-b15f-7cb930f11af9","metadata":{"id":"e14d3bf5-37ef-4d63-b15f-7cb930f11af9","executionInfo":{"status":"aborted","timestamp":1768211226258,"user_tz":300,"elapsed":2,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["tokenizer = BasicTokenizer(266)\n","print(\"The text to encode is the following:\", text)\n","tokenizer.train(text, verbose=True)\n","\n","encoded = tokenizer.encode(\"hello üòÑ students\")\n","decoded = tokenizer.decode(encoded)\n","\n","print(encoded)\n","print(decoded)"]},{"cell_type":"code","execution_count":null,"id":"ee801642-77ac-43e3-b676-1b4d88ad18af","metadata":{"id":"ee801642-77ac-43e3-b676-1b4d88ad18af","executionInfo":{"status":"aborted","timestamp":1768211226259,"user_tz":300,"elapsed":2,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["tokenizer = BasicTokenizer(266)\n","textTest = \"hello everyone\"\n","print(\"The text to encode is the following:\", textTest)\n","tokenizer.train(textTest, verbose=True)\n","\n","encoded = tokenizer.encode(textTest)\n","decoded = tokenizer.decode(encoded)\n","\n","print(encoded)\n","print(decoded)"]},{"cell_type":"markdown","id":"6b4b4df3-b4dd-4664-b289-047a5d6abe58","metadata":{"id":"6b4b4df3-b4dd-4664-b289-047a5d6abe58"},"source":["## 10. Now , your turn!\n","\n","Try the following:\n","\n","1. Change `vocab_size` and measure compression\n","2. Train on:\n","   - English text\n","   - Code\n","   - Emojis\n","3. Print the first 20 learned merges\n","4. Compare with character-level tokenization\n","\n","‚úçÔ∏è Write short answers below each experiment."]},{"cell_type":"code","execution_count":null,"id":"30acfdb6-92f2-41e8-bffc-386fca470397","metadata":{"id":"30acfdb6-92f2-41e8-bffc-386fca470397","executionInfo":{"status":"aborted","timestamp":1768211226261,"user_tz":300,"elapsed":4,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["tokenizer = BasicTokenizer(280)\n","textTest = \"‚úçÔ∏è Write short answers below each experiment.\"\n","print(\"The text to encode is the following:\", textTest)\n","tokenizer.train(textTest, verbose=True)\n","\n","encoded = tokenizer.encode(textTest)\n","decoded = tokenizer.decode(encoded)\n","\n","print(encoded)\n","print(decoded)"]},{"cell_type":"markdown","id":"3951c123-8399-4e6d-a34f-4a1c55e8a843","metadata":{"id":"3951c123-8399-4e6d-a34f-4a1c55e8a843"},"source":["## 10. Why This Matters for Language Models\n","\n","- Tokens ‚Üí embeddings\n","- Fewer tokens ‚Üí longer context\n","- Tokenization is the **first inductive bias** of a Transformer\n","- Good tokenization matters more when the model is small.\n","\n","This tokenizer can now be plugged directly into a GPT training loop.\n","\n","<a href=\"https://tiktokenizer.vercel.app/\" >\n","    <img src=\"Tiktokenizer.png\" >\n","</a>"]},{"cell_type":"markdown","id":"4c08948b-4e9e-4ed5-9134-ef1ea162e637","metadata":{"id":"4c08948b-4e9e-4ed5-9134-ef1ea162e637"},"source":["# Regex Tokenizer"]},{"cell_type":"markdown","id":"d3920c2e-7761-4b4c-b970-4dc72b884e10","metadata":{"id":"d3920c2e-7761-4b4c-b970-4dc72b884e10"},"source":["### 1. Why RegexTokenizer Exists\n","\n","Your current basicTokenizer trains BPE on raw byte streams:\n","\n","```\n","text ‚Üí bytes ‚Üí BPE merges\n","```\n","\n","That works ‚Äî but it has drawbacks:\n","\n","- BPE may merge across semantic boundaries\n","\n","- Punctuation, whitespace, and numbers get mixed\n","\n","- Training is slower and noisier\n","\n","- Tokens can become syntactically awkward\n","\n","RegexTokenizer fixes this by adding structure before BPE."]},{"cell_type":"markdown","id":"39a4e824-1f80-4dd1-8950-5d5d9d7e7705","metadata":{"id":"39a4e824-1f80-4dd1-8950-5d5d9d7e7705"},"source":["### 2Ô∏è. Core Idea (One Sentence)\n","\n","RegexTokenizer first splits text into meaningful chunks using regex, then applies BPE inside each chunk independently.\n","\n","This is exactly how GPT-2 / GPT-3 style tokenizers work."]},{"cell_type":"markdown","id":"3786db34-773b-421b-af0d-753eb716dd30","metadata":{"id":"3786db34-773b-421b-af0d-753eb716dd30"},"source":["### 3. High-Level Pipeline\n","\n","```\n","Text\n"," ‚Üì\n","Regex split (words, numbers, punctuation, spaces)\n"," ‚Üì\n","Each chunk ‚Üí UTF-8 bytes\n"," ‚Üì\n","Byte Pair Encoding (BPE)\n"," ‚Üì\n","Final token IDs\n","```\n","\n","So instead of training BPE on everything, we train it on pre-segmented text units."]},{"cell_type":"markdown","id":"580b734a-2a3f-47dd-9d9a-31c7dd888c86","metadata":{"id":"580b734a-2a3f-47dd-9d9a-31c7dd888c86"},"source":["Let‚Äôs decode what it does.\n","\n","What the Regex Captures\n","Pattern\tMeaning\n","\n","| Pattern    | Meaning |\n","| -------- | ------- |\n","| \\p{L}+  | Letters (words)    |\n","| \\p{N}+ | Numbers     |\n","| [^ \\s\\p{L}\\p{N}]+    | Punctuation / symbols    |\n","| \\s+    | Whitespace    |\n","| 's, 't, etc.    | English contractions   |"]},{"cell_type":"code","execution_count":null,"id":"04d5a989-c47f-4510-8d2c-fafb3e78c22c","metadata":{"id":"04d5a989-c47f-4510-8d2c-fafb3e78c22c","executionInfo":{"status":"aborted","timestamp":1768211226262,"user_tz":300,"elapsed":397,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["import regex as re\n","\n","pattern = re.compile(\n","    r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\", # NOTE: this split pattern deviates from GPT-4 in that it is used \\p{N}{1,2} instead of \\p{N}{1,3}\n","# I did this because I didn't want to \"waste\" too many tokens on numbers for smaller vocab sizes.\n","# I haven't validated that this is actually a good idea, TODO.\n","    re.UNICODE\n",")"]},{"cell_type":"code","execution_count":null,"id":"18946319-155a-432c-a7df-f3ca25364529","metadata":{"id":"18946319-155a-432c-a7df-f3ca25364529","executionInfo":{"status":"aborted","timestamp":1768211226262,"user_tz":300,"elapsed":395,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["text = \"Hello üòÑ students\"\n","print(text)\n","print(\"-\"*50)\n","print(\"Splitting Regex Pattern Capture:\\n\", re.findall(pattern, text))"]},{"cell_type":"markdown","id":"803b7041-112d-4265-b51f-d3a852671006","metadata":{"id":"803b7041-112d-4265-b51f-d3a852671006"},"source":["#### ! Important: Spaces are kept, not discarded."]},{"cell_type":"code","execution_count":null,"id":"eccfc9f1-c1e0-4d10-a144-a1978a5378d1","metadata":{"id":"eccfc9f1-c1e0-4d10-a144-a1978a5378d1","executionInfo":{"status":"aborted","timestamp":1768211226263,"user_tz":300,"elapsed":394,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["text = \"ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\"\n","print(text)\n","print(\"-\"*50)\n","re.findall(pattern, text)"]},{"cell_type":"markdown","id":"ec2e76d5-ef98-478f-9c22-4cc08d819698","metadata":{"id":"ec2e76d5-ef98-478f-9c22-4cc08d819698"},"source":["### 3. Why This Matters Before BPE\n","Without Regex (Your Basic Tokenizer)\n","\n","BPE might merge:\n","\n","```\n","\n","\"o \" + \"t\" ‚Üí \"o t\"\n","\n","```\n","\n","Which is meaningless.\n","\n","With RegexTokenizer\n","\n","BPE operates inside units like:\n","\n","- \"hello\"\n","\n","- \" there\"\n","\n","- \"!\"\n","\n","This ensures:\n","\n","- Cleaner merges\n","\n","- Faster convergence\n","\n","- More interpretable tokens\n"]},{"cell_type":"code","execution_count":null,"id":"20e4a9b4-3de3-43a2-9231-50a41ff9aec1","metadata":{"id":"20e4a9b4-3de3-43a2-9231-50a41ff9aec1","executionInfo":{"status":"aborted","timestamp":1768211226264,"user_tz":300,"elapsed":393,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["sample = \"Hello there! I'm 26. üòÑ\\nNew line.\"\n","chunks = re.findall(pattern, sample)\n","chunks"]},{"cell_type":"code","execution_count":null,"id":"478d0af8","metadata":{"id":"478d0af8","executionInfo":{"status":"aborted","timestamp":1768211226264,"user_tz":300,"elapsed":391,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["vocabulary = {i: bytes([i]) for i in range(256)}\n","print(vocabulary[97], vocabulary[97].decode(\"utf-8\"))  # 'a'\n","print(list(\"üòÑ\".encode(\"utf-8\")))"]},{"cell_type":"markdown","id":"2d4ec971","metadata":{"id":"2d4ec971"},"source":["### BPE primitives: count adjacent pairs + merge\n","\n","BPE repeatedly merges the most frequent adjacent pair into a new token id.\n"]},{"cell_type":"code","execution_count":null,"id":"4c2cea36","metadata":{"id":"4c2cea36","executionInfo":{"status":"aborted","timestamp":1768211226265,"user_tz":300,"elapsed":392,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["def get_pair(ids, counts):\n","    for pair in zip(ids, ids[1:]):\n","        counts[pair] = counts.get(pair, 0) + 1\n","    return counts\n","\n","def merge(ids, pair, index):\n","    '''\n","    This function will iterate over ids and every time\n","    it sees a instance of pair, it will take that pair\n","    and instead put index , then it will return the list\n","    merge()\n","    list = [1,2, 3, 4, 1, 2]\n","    merge(list, (1,2). 257)\n","    list = [257, 3, 4, 257, 3]\n","    '''\n","\n","    new_ids = []\n","    i = 0\n","    while i < len(ids):\n","        if i <len(ids) - 1 and  (ids[i], ids[i+1]) == pair:\n","            new_ids.append(index)\n","            i += 2\n","        else:\n","            new_ids.append(ids[i])\n","            i += 1\n","    return new_ids"]},{"cell_type":"markdown","id":"d0bb8fcf","metadata":{"id":"d0bb8fcf"},"source":["### 4. RegexTokenizer overview\n","\n","Training:\n","1. Split text into regex chunks\n","2. Convert each chunk to UTF-8 bytes (list of ints)\n","3. Count pairs across **all chunks**\n","4. Merge the most frequent pair across **all chunks**\n","5. Repeat until reaching vocab_size\n","\n","Encoding:\n","1. Split input text into regex chunks\n","2. Encode each chunk with learned merges\n","3. Concatenate token ids\n","\n","Decoding:\n","- Map ids ‚Üí bytes ‚Üí UTF-8 string"]},{"cell_type":"code","execution_count":null,"id":"58c120ce","metadata":{"id":"58c120ce","executionInfo":{"status":"aborted","timestamp":1768211226266,"user_tz":300,"elapsed":392,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["import pickle"]},{"cell_type":"code","execution_count":null,"id":"08d8af5a","metadata":{"id":"08d8af5a","executionInfo":{"status":"aborted","timestamp":1768211226266,"user_tz":300,"elapsed":392,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["\n","class regexTokenizer:\n","\n","    def __init__(self, vocab_size, pattern):\n","\n","        self.vocab_size = vocab_size\n","        self.vocabulary = {i : bytes([i]) for i in range(256)}\n","        self.merges = {}\n","        self.pattern = re.compile(pattern)\n","\n","    def train(self, text, verbose = False):\n","        # Encode the text\n","        # Iterate over text, self.vocab_size - 256 times\n","        # count all of the pairs in a dictionary\n","        # choose the pair with the highest frequency\n","        # merge that pair as a new token\n","        # add that token to the vocab\n","        # {256: byte_string}\n","        # add to self.merges = {byte_string: 256}\n","\n","        assert self.vocab_size > 256\n","        number_merges = self.vocab_size - 256\n","\n","        text_chunks = re.findall(self.pattern, text)\n","        encoded_chunks = [list(text_chunk.encode('utf-8')) for text_chunk in text_chunks]\n","\n","        length_initial = sum([len(encoded_chunk) for encoded_chunk in encoded_chunks])\n","\n","        for i in range(number_merges):\n","            pairs = {}\n","            for encoded_chunk in encoded_chunks:\n","                pairs = get_pair(encoded_chunk, pairs)\n","\n","            pair = max(pairs, key = pairs.get)\n","            index = 256 + i\n","            encoded_chunks = [merge(encoded_chunk,pair,index) for encoded_chunk in encoded_chunks]\n","            self.merges[pair] = index\n","            self.vocabulary[index] = self.vocabulary[pair[0]] + self.vocabulary[pair[1]]\n","            #print(sorted( [(v, k) for k,v in pairs.items()], reverse = True) [:10])\n","\n","        if verbose:\n","            length_final = sum([len(encoded_chunk) for encoded_chunk in encoded_chunks])\n","            compression = length_initial/length_final\n","            print(length_initial, length_final)\n","            print(compression)\n","\n","    def encode(self, text):\n","\n","        text_chunks = re.findall(self.pattern, text)\n","        encoded_text = []\n","\n","        for text_chunk in text_chunks:\n","            encoded_chunk = self.encode_chunk(text_chunk)\n","            encoded_text.extend(encoded_chunk)\n","        return encoded_text\n","\n","    def encode_chunk(self, text):\n","        '''\n","        self.merges is important here\n","\n","        we get text, and then we convert that text to byte strings, then to integers\n","        and then we iterate over the text until all pairs of\n","        merges that are possible under the trained tokenizer\n","        have been completed\n","\n","        '''\n","\n","        ids = list(text.encode('utf-8'))\n","\n","        for pair, index in self.merges.items():\n","            ids = merge(ids, pair, index)\n","\n","        return ids\n","\n","    def decode(self, ids):\n","        '''\n","        decode gets ids\n","        1. convert the ids to their byte strings\n","        2. convert the byte strings to strings via the vocabulary\n","        3. then return the decoded_text\n","        '''\n","\n","        byte_strings = b''.join([bytes(self.vocabulary[i]) for i in ids])\n","        decoded_text =  byte_strings.decode('utf-8')\n","        return decoded_text\n","\n","    def save(self, path):\n","        with open(path, \"wb\") as file:\n","            pickle.dump(\n","                {\n","                    \"merges\": self.merges,\n","                    \"vocabulary\": self.vocabulary,\n","                    \"pattern\": self.pattern\n","                },\n","                file\n","            )\n","\n","    @classmethod\n","    def load(cls, path):\n","        tokenizer = cls(300, pattern)\n","\n","        with open(path , \"rb\") as file:\n","            data = pickle.load(file)\n","            tokenizer.merges = data[\"merges\"]\n","            tokenizer.vocabulary = data[\"vocabulary\"]\n","            tokenizer.pattern = data[\"pattern\"]\n","        return tokenizer\n"]},{"cell_type":"code","execution_count":null,"id":"389956f7","metadata":{"id":"389956f7","executionInfo":{"status":"aborted","timestamp":1768211226267,"user_tz":300,"elapsed":393,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["tokenizer = regexTokenizer(300, pattern)"]},{"cell_type":"code","execution_count":null,"id":"411425bd","metadata":{"id":"411425bd","executionInfo":{"status":"aborted","timestamp":1768211226267,"user_tz":300,"elapsed":390,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["text = \"ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\"\n","tokenizer.train(text, True)"]},{"cell_type":"markdown","source":["### 5. Exercise (10 minutes)\n","1. Chunk inspection:\n","   - Print chunks for:\n","   - English sentence\n","   - Code snippet\n","   - Emoji-heavy text\n","2. Compression study:\n","   - Train with vocab_size in {300, 500, 1000}\n","   - Report compression ratio\n","3. Token boundary study:\n","   - Compare tokenization for:\n","   - \"hello there\"\n","   - \"hello there\" (double space)\n","   - \"hello\\nthere\"\n","4. Compare the tokenization between the basic Tokenizer and regex Tokenizer\""],"metadata":{"id":"a-Nce0Kwmm1I"},"id":"a-Nce0Kwmm1I"},{"cell_type":"code","source":["# TO DO"],"metadata":{"id":"UDSkhrDZm5os"},"id":"UDSkhrDZm5os","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"f9c02d8f-1ecc-426e-9e52-25383431327f","metadata":{"id":"f9c02d8f-1ecc-426e-9e52-25383431327f"},"source":["# Nanochat Tokenizer\n","\n","We move from educational tokenizers to the **final tokenizer**\n","used in NanoChat.\n","\n","You will learn:\n","- How GPT-style tokenizers handle *chat*\n","- What special tokens are and why they matter\n","- How conversations are rendered into `(input_ids, loss_mask)`\n","- How this enables supervised fine-tuning (SFT)"]},{"cell_type":"code","execution_count":null,"id":"fe6f1bf1-6ebc-4543-b0b0-c0de817e2d56","metadata":{"id":"fe6f1bf1-6ebc-4543-b0b0-c0de817e2d56","executionInfo":{"status":"aborted","timestamp":1768211226268,"user_tz":300,"elapsed":391,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["# First we need to download the weights https://huggingface.co/karpathy/nanochat-d32/tree/main\n","# Put the tokenizer.pkl in ~/.cache/nanochat/tokenizer directory"]},{"cell_type":"code","source":["!git clone https://github.com/karpathy/nanochat.git\n"],"metadata":{"id":"AXv9NVuMyo9x","executionInfo":{"status":"aborted","timestamp":1768211226269,"user_tz":300,"elapsed":390,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"id":"AXv9NVuMyo9x","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd nanochat"],"metadata":{"id":"70KVgkOb0Mg-","executionInfo":{"status":"aborted","timestamp":1768211226270,"user_tz":300,"elapsed":390,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"id":"70KVgkOb0Mg-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove ipykernel and add\n","#[tool.setuptools.packages.find]\n","#where = [\".\"]\n","#include = [\"nanochat*\"]\n","#exclude = [\"class*\", \"dev*\"]\n","# to pyproject.toml\n","!pip install -e ."],"metadata":{"id":"E5j0LT7H3qy6","executionInfo":{"status":"aborted","timestamp":1768211226271,"user_tz":300,"elapsed":389,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"id":"E5j0LT7H3qy6","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"326ed9c5-b1e9-4516-8b7c-731402cb49c8","metadata":{"id":"326ed9c5-b1e9-4516-8b7c-731402cb49c8","executionInfo":{"status":"aborted","timestamp":1768211226272,"user_tz":300,"elapsed":390,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["from nanochat.tokenizer import get_tokenizer"]},{"cell_type":"code","source":["%cd /root"],"metadata":{"id":"Z_F3jHrY4zQO","executionInfo":{"status":"aborted","timestamp":1768211226273,"user_tz":300,"elapsed":389,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"id":"Z_F3jHrY4zQO","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%mkdir /root/.cache/nanochat/tokenizer/"],"metadata":{"id":"VBnCm1np4122","executionInfo":{"status":"aborted","timestamp":1768211226389,"user_tz":300,"elapsed":503,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"id":"VBnCm1np4122","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /root/.cache/nanochat/tokenizer"],"metadata":{"id":"kn8gd5tf4-PT","executionInfo":{"status":"aborted","timestamp":1768211226391,"user_tz":300,"elapsed":504,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"id":"kn8gd5tf4-PT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import hf_hub_download\n","# Download the specific file and get its local file path\n","tokenizer_pkl_path = hf_hub_download(repo_id=\"karpathy/nanochat-d34\", filename=\"tokenizer.pkl\")\n","tokenizer_pt_path = hf_hub_download(repo_id=\"karpathy/nanochat-d34\", filename=\"token_bytes.pt\")"],"metadata":{"id":"RVxEXyRx5Kmg","executionInfo":{"status":"aborted","timestamp":1768211226391,"user_tz":300,"elapsed":502,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"id":"RVxEXyRx5Kmg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer_pkl_path"],"metadata":{"id":"NA7eTPA-5bM9","executionInfo":{"status":"aborted","timestamp":1768211226392,"user_tz":300,"elapsed":501,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"id":"NA7eTPA-5bM9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cp '/root/.cache/huggingface/hub/models--karpathy--nanochat-d34/snapshots/c48357d43863a3a6cdc5f5db5b4ec5964e4192d6/tokenizer.pkl' '/root/.cache/nanochat/tokenizer/tokenizer.pkl'"],"metadata":{"id":"9mr35FsV5Xwh","executionInfo":{"status":"aborted","timestamp":1768211226393,"user_tz":300,"elapsed":502,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"id":"9mr35FsV5Xwh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/nanochat/\n","%ls"],"metadata":{"id":"OXE5XiTn5qAm","executionInfo":{"status":"aborted","timestamp":1768211226393,"user_tz":300,"elapsed":500,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"id":"OXE5XiTn5qAm","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"303984c8-0355-480e-b9f1-4c7df14560bc","metadata":{"id":"303984c8-0355-480e-b9f1-4c7df14560bc","executionInfo":{"status":"aborted","timestamp":1768211226394,"user_tz":300,"elapsed":499,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["tokenizer = get_tokenizer()\n","print(type(tokenizer))"]},{"cell_type":"markdown","id":"4c633459-e121-436c-976b-65f52c90d811","metadata":{"id":"4c633459-e121-436c-976b-65f52c90d811"},"source":["### 1. Special Tokens\n","\n","Chat models need *control tokens* to:\n","- separate user vs assistant\n","- delimit messages\n","- support tools (python blocks, outputs)"]},{"cell_type":"code","execution_count":null,"id":"b5b607d4-0429-4fbc-88fa-7fd814af1f67","metadata":{"id":"b5b607d4-0429-4fbc-88fa-7fd814af1f67","executionInfo":{"status":"aborted","timestamp":1768211226395,"user_tz":300,"elapsed":498,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["special_tokens = tokenizer.get_special_tokens()\n","special_tokens"]},{"cell_type":"code","execution_count":null,"id":"84dec289-dd9b-40d8-bfe5-15e6e7ddb626","metadata":{"id":"84dec289-dd9b-40d8-bfe5-15e6e7ddb626","executionInfo":{"status":"aborted","timestamp":1768211226396,"user_tz":300,"elapsed":497,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["# Lets encode these special tokens\n","for tok in special_tokens:\n","    print(f\"{tok:25s} ‚Üí id {tokenizer.encode_special(tok)}\")"]},{"cell_type":"code","execution_count":null,"id":"731db13e-c0f9-453d-a9b9-b0c414e35722","metadata":{"id":"731db13e-c0f9-453d-a9b9-b0c414e35722","executionInfo":{"status":"aborted","timestamp":1768211226396,"user_tz":300,"elapsed":496,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["# Lets decode it again to see if everything works well\n","encoded_special_tokens = [ tokenizer.encode_special(tok) for tok in list(special_tokens) ]\n","for id in encoded_special_tokens:\n","    print(f\"{id} ‚Üí id {tokenizer.decode([id])}\")"]},{"cell_type":"code","execution_count":null,"id":"0b06c865-c9e9-4fc5-a01e-c3a77e0b734b","metadata":{"id":"0b06c865-c9e9-4fc5-a01e-c3a77e0b734b","executionInfo":{"status":"aborted","timestamp":1768211226397,"user_tz":300,"elapsed":495,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["# Lets try with the text we already worked\n","text = \"ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\"\n","ids = tokenizer.encode(text)\n","decoded = tokenizer.decode(ids)\n","\n","ids, decoded"]},{"cell_type":"code","execution_count":null,"id":"e1d532d9-71cb-40aa-bad8-0e8cb7ec85aa","metadata":{"id":"e1d532d9-71cb-40aa-bad8-0e8cb7ec85aa","executionInfo":{"status":"aborted","timestamp":1768211226397,"user_tz":300,"elapsed":495,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["conversation = {\n","    \"messages\": [\n","        {\"role\": \"user\", \"content\": \"What is a transformer?\"},\n","        {\"role\": \"assistant\", \"content\": \"A transformer is a neural network based on attention.\"}\n","    ]\n","}"]},{"cell_type":"code","execution_count":null,"id":"d9c04aad-df97-411d-bad4-f9ffe0f9aeb9","metadata":{"id":"d9c04aad-df97-411d-bad4-f9ffe0f9aeb9","executionInfo":{"status":"aborted","timestamp":1768211226398,"user_tz":300,"elapsed":494,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["ids, loss_mask = tokenizer.render_conversation(conversation)\n","\n","print(\"Number of tokens:\", len(ids))\n","print(\"Loss tokens:\", sum(loss_mask))"]},{"cell_type":"code","execution_count":null,"id":"6e5b1be7-ea68-402b-9073-f8f18f877b94","metadata":{"id":"6e5b1be7-ea68-402b-9073-f8f18f877b94","executionInfo":{"status":"aborted","timestamp":1768211226399,"user_tz":300,"elapsed":495,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["def printUserAssistantType(mask):\n","    if mask == 0:\n","        return \"User\"\n","    else:\n","        return \"Assistant\""]},{"cell_type":"code","execution_count":null,"id":"5984f92d-c15d-4e73-aeea-34bf4a55b323","metadata":{"id":"5984f92d-c15d-4e73-aeea-34bf4a55b323","executionInfo":{"status":"aborted","timestamp":1768211226399,"user_tz":300,"elapsed":493,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["decoded_tokens = [tokenizer.decode([i]) for i in ids]\n","\n","for t, m in zip(decoded_tokens, loss_mask):\n","    print(f\"{repr(t):20s}  mask={m} {printUserAssistantType(m)}\")"]},{"cell_type":"markdown","id":"c37684ee-9f41-4a85-a850-fa135c8629f2","metadata":{"id":"c37684ee-9f41-4a85-a850-fa135c8629f2"},"source":["### 2. Why Masking?\n","\n","Without masking:\n","- model would be trained to predict user prompts\n","- learning becomes unstable\n","- chat behavior degrades\n","\n","Masking enforces:\n","P(assistant | user)"]},{"cell_type":"markdown","id":"55d53edf-210f-4cd6-8b17-4c970e5d5403","metadata":{"id":"55d53edf-210f-4cd6-8b17-4c970e5d5403"},"source":["### 3. Exercise (15 minutes)\n","1. Create a 3-turn conversation\n","2. Render it\n","3. Count how many tokens are supervised\n","4. Inspect where assistant supervision starts"]},{"cell_type":"code","execution_count":null,"id":"91b6a4fe-dd46-407a-b357-98837611c4e7","metadata":{"id":"91b6a4fe-dd46-407a-b357-98837611c4e7","executionInfo":{"status":"aborted","timestamp":1768211226400,"user_tz":300,"elapsed":494,"user":{"displayName":"RENATO AAR√ìN CASTRO CRUZ","userId":"03533247398983604457"}}},"outputs":[],"source":["## TO DO"]},{"cell_type":"markdown","id":"09aad174-9433-4fa5-ad76-7fd1e03afa5c","metadata":{"id":"09aad174-9433-4fa5-ad76-7fd1e03afa5c"},"source":["## Next Step\n","\n","We now have:\n","- Token IDs\n","- Loss masks\n","\n","Next:\n","‚û° Let's understand GPT"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"panel-cell-order":["f01e1d45-3aab-465c-9480-fc7dc5106e69","306cd8b2-2a76-422a-b78c-f181e1a64620","bdae7223-9d36-4c45-93a2-bf4fd6a74bcd"],"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}