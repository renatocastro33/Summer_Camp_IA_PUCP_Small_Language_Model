{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ec2e79-deab-4431-975e-4bf00540cd56",
   "metadata": {},
   "source": [
    "## Understanding GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa25c289-09c5-47ed-bdc9-822b8069ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import  torch.nn.functional as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05ef605d-f830-4248-95c4-d87c707cb334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from turtle import forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29141a16-68ed-4530-88b2-26dd04955b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanochat.common import get_dist_info\n",
    "from nanochat.muon import Muon, DistMuon\n",
    "from nanochat.adamw import DistAdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b76ad9c-f9a1-4f62-a52c-79d5cf483b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    seq_len = 1024\n",
    "    vocab_size = 50304\n",
    "    n_layer = 12\n",
    "    n_head = 66\n",
    "    n_kv_head = 6\n",
    "    emb_dim = 768\n",
    "    head_dim = emb_dim/n_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b12def3d-b3cf-4b55-adda-3080540d9587",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def RMS(x, epsilon):\n",
    "    return x / torch.sqrt(torch.mean(x**2, dim =-1, keepdim = True) + epsilon)\n",
    "\n",
    "def relu(x):\n",
    "    return x * ( x > 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da9fda77-7a19-4865-8fc9-8898f6b40750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config, layer_idx) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.n_head = config.n_head\n",
    "        self.model_dim = config.model_dim\n",
    "        self.head_dim = self.model_dim // self.n_head\n",
    "        self.n_kv_head = config.n_kv_head\n",
    "        assert self.model_dim % self.n_head == 0\n",
    "        assert self.n_kv_head <= self.n_head\n",
    "        assert self.n_head % self.n_kv_head == 0\n",
    "        self.q = nn.Linear(self.model_dim , self.n_head * self.head_dim, bias = False)\n",
    "        self.k = nn.Linear(self.model_dim , self.n_head * self.head_dim, bias = False)\n",
    "        self.v = nn.Linear(self.model_dim , self.n_head * self.head_dim, bias = False)\n",
    "        self.proj == nn.Linear(self.model_dim , self.model_dim, bias = False)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(config.emb_dim, 4*config.emb_dim, bias = False)\n",
    "        self.down_proj = nn.Linear(config.emb_dim * 4, config.emb_dim, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj((relu(self.uproj(x)).square()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "765fc136-6c8b-4530-b5dc-6dec8132ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config, layer_idx) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(config, layer_idx)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(RMS(x))\n",
    "        x = x + self.mlp(RMS(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bbc27ee-3796-4d0f-a6cf-de552a5e3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.emb_dim)\n",
    "        self.blocks = [Block(config,layer_idx) for layer_idx in range(config.n_layer)]\n",
    "        self.lm_head(config.seq_len, config.vocab_size, bias = False)\n",
    "        cos, sin = self._precompute_rotary_embeddings(config.seq_len*10, config.head_dim)\n",
    "        self.register_buffer(\"cos\", cos, persistent = True)\n",
    "        self.register_buffer(\"sin\", sin, persistent = True)\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def get_device(self):\n",
    "        return self.wte.weight.device\n",
    "\n",
    "    def _precompute_rotary_embeddings(self, seq_len, head_dim, base= 1000, device = None):\n",
    "        if device is None:\n",
    "            device= self.get_device()\n",
    "        # Stride over the channels\n",
    "        \n",
    "        channel_range = torch.arange(0, head_dim, 2, dtype = torch.float32, device = device)\n",
    "        inv_freq = 1.0/(base**(2*channel_range/head_dim))\n",
    "        token_index = torch.arange(seq_len, dtype = torch.float32, device = device)\n",
    "        \n",
    "        freqs = torch.outer(token_index, inv_freq)\n",
    "        cos, sin = torch.cos(freqs), torch.sin(freq)\n",
    "        # shape = (b x seq_len x n_heads x head_dim/2)\n",
    "        cos, sin = cos.bfloat16(), sin.bfloat16()\n",
    "        cos = torch.unsqueeze(torch.unsqueeze(cos, 0), 2)\n",
    "        sin = torch.unsqueeze(torch.unsqueeze(sin, 0), 2)\n",
    "        \n",
    "        return cos, sin\n",
    "        \n",
    "        \n",
    "    @torch.inference_mode()\n",
    "    def generate(self, tokens, max_tokens, temperature, top_k = None, seed = 42):\n",
    "        assert isinstance(tokens, list)\n",
    "        device= self.get_device()\n",
    "        rng = None\n",
    "        if temperature > 0:\n",
    "            rng = torch.Generator(device = device)\n",
    "            rng.manual_seed = seed\n",
    "        ids = torch.tensor([tokens], dtype = torch.long, device = device)\n",
    "        for _ in range(max_tokens):\n",
    "            \n",
    "            logits = self.forward(ids)\n",
    "            # Batch x Seq_Len x Vocab_Size\n",
    "            logits = logits[:, -1, : ]\n",
    "            if top_k is not None:\n",
    "                # Batch x 1 x k\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits = torch.where(logits < v[:, [-1]], -float('inf'), logits)\n",
    "            if temperature > 0:\n",
    "                logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim = -1)\n",
    "                next_ids = torch.multinomial(probs, num_samples = 1, generator=rng)\n",
    "            else:\n",
    "                next_ids = torch.argmax(logits, dim=-1, keep = True)\n",
    "            ids = torch.cat( (ids, next _ids), 1)\n",
    "            token = next_ids.item()\n",
    "            yield token\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8492abf-2d4f-4aaf-bc10-2b8c422e9373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9453d08-7141-41c4-b507-63f6ae219813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad667ae2-77c5-4fce-a22f-8bcdbf97d21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cda73c6b-dab5-4fc4-8036-94692845f1b3",
   "metadata": {},
   "source": [
    "# Training GPT\n",
    "\n",
    "In this notebook you will:\n",
    "- Inspect NanoChat's GPT architecture\n",
    "- Run a forward pass\n",
    "- Compute masked language modeling loss\n",
    "- Train a tiny chat model for a few steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd699321-e274-4970-ad25-1673b4ab61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanochat.gpt import GPT, GPTConfig\n",
    "from nanochat.tokenizer import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42654b28-3b40-42b1-95ac-10f4280ca8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
