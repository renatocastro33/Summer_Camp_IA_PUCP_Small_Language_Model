{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01e1d45-3aab-465c-9480-fc7dc5106e69",
   "metadata": {
    "panel-layout": {
     "height": 60.59375,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# Small Language Workshop Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cd8b2-2a76-422a-b78c-f181e1a64620",
   "metadata": {
    "panel-layout": {
     "height": 220.421875,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# Tokenization from Scratch: Building a Byte Pair Encoder (BPE)\n",
    "\n",
    "In this notebook, we will implement a **byte-level tokenizer** similar to the one used in GPT-style models.\n",
    "\n",
    "By the end, you will:\n",
    "- Understand why tokenization is necessary\n",
    "- Implement Byte Pair Encoding from scratch\n",
    "- Train a tokenizer on real Unicode text\n",
    "- Encode and decode text losslessly\n",
    "- Understand why tokenization matters for Small Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae7223-9d36-4c45-93a2-bf4fd6a74bcd",
   "metadata": {
    "panel-layout": {
     "height": 223.234375,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## 1. Why Tokenization?\n",
    "\n",
    "Neural networks operate on **numbers**, but language is **text**.\n",
    "\n",
    "Key challenges:\n",
    "- Variable-length sequences\n",
    "- Unicode & emojis\n",
    "- Finite vocabulary\n",
    "- Efficiency (context length matters!)\n",
    "\n",
    "ğŸ’¡ **Key idea:** Tokenization is a *compression problem*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9107a-b677-4b2a-aab2-a18757cce42b",
   "metadata": {},
   "source": [
    "<img src='Neural_Network.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c1823-9a51-408f-81b5-3adf864b17d8",
   "metadata": {},
   "source": [
    "## 2. Text â†’ Bytes\n",
    "\n",
    "Instead of characters or words, GPT-style models operate on **bytes**.\n",
    "\n",
    "Why?\n",
    "- Every string can be represented as bytes\n",
    "- No unknown tokens\n",
    "- Unicode-safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e5f92a-3350-4be8-b11d-6162efcfda79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: hello ğŸ˜„ students\n",
      "UTF-8 Encoding: b'hello \\xf0\\x9f\\x98\\x84 students'\n",
      "List Encoded UTF-8: [104, 101, 108, 108, 111, 32, 240, 159, 152, 132, 32, 115, 116, 117, 100, 101, 110, 116, 115]\n"
     ]
    }
   ],
   "source": [
    "text = \"hello ğŸ˜„ students\"\n",
    "encoded = text.encode(\"utf-8\")\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"UTF-8 Encoding:\", encoded) # Bytes values\n",
    "print(\"List Encoded UTF-8:\", list(encoded)) # Decimal Values in Hexadecimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d4f8c-61d0-44c4-8b7d-8ffbda3ae31d",
   "metadata": {},
   "source": [
    "ğŸ§  **Think**\n",
    "- Why does the emoji produce multiple numbers?\n",
    "- Why might this be better than character-level tokenization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7b3d8-be11-48b0-8327-87e2e91fecf8",
   "metadata": {},
   "source": [
    "## 3. Initial Vocabulary\n",
    "\n",
    "We start with a vocabulary of **256 tokens**, one for each possible byte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b970630-c84a-4f9c-887e-83512313fe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a' a\n",
      "b'h' b'h'\n",
      "b'\\xf0' b'\\xf0'\n"
     ]
    }
   ],
   "source": [
    "vocab = {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "# Sanity check\n",
    "print(vocab[97], vocab[97].decode(\"utf-8\"))  # 'a'\n",
    "\n",
    "# So we can take the previous values of the string \"hello ğŸ˜„ students\"\n",
    "print(vocab[104], bytes([encoded[0]]))  # It should match\n",
    "print(vocab[240], bytes([encoded[6]]))  # It should match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74347d31-bfd2-414e-890e-ff13222be5dc",
   "metadata": {},
   "source": [
    "## 4. Counting Adjacent Pairs\n",
    "\n",
    "Byte Pair Encoding works by repeatedly merging the **most frequent adjacent pair**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0560b4d0-8fd9-4167-84ce-6c4dc2b62570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        # TODO: count how many times each pair appears\n",
    "        counts[pair] = counts.get(pair, 0)  + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c72a792a-e430-497b-ba57-54495569e0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 2): 2, (2, 3): 1, (3, 1): 1}\n"
     ]
    }
   ],
   "source": [
    "ids = [1, 2, 3, 1, 2]\n",
    "print(get_pair(ids))\n",
    "\n",
    "# (1,1) or (2,2) does not count because the elements are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c97e5e-83bf-47d9-9240-e70ae279d7a5",
   "metadata": {},
   "source": [
    "## 5. Merging Pairs\n",
    "\n",
    "When we merge a pair `(a, b)` into a new token `k`,\n",
    "we replace all occurrences of `(a, b)` with `k`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329cc33f-251c-4b8a-b180-6ecd6f90a87f",
   "metadata": {},
   "source": [
    "<img src = 'BPE_Algorithm.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62ee8903-1912-45f5-8749-ec4c084fb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, index):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and (ids[i], ids[i+1]) == pair:\n",
    "            new_ids.append(index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed48a2a6-fe23-4fe8-a16c-87303ab69ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[256, 3, 256]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge([1, 2, 3, 1, 2], (1, 2), 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124acf4b-018c-4d36-a3dc-6c87fe002542",
   "metadata": {},
   "source": [
    "What happened to sequence length? Why is this useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162fa746-9ecf-4a1b-b560-acb1f9ad42f6",
   "metadata": {},
   "source": [
    "## 6. Training the Tokenizer\n",
    "\n",
    "We will now repeatedly:\n",
    "1. Count all adjacent pairs\n",
    "2. Select the most frequent one\n",
    "3. Merge it into a new token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc14afb-3418-4758-8987-ec6e3bf69c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocabulary = {i: bytes([i]) for i in range(256)}\n",
    "        self.merges = {}\n",
    "\n",
    "    def train(self, text, verbose=False):\n",
    "        assert self.vocab_size > 256\n",
    "\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "        initial_len = len(ids)\n",
    "\n",
    "        for i in range(self.vocab_size - 256):\n",
    "            pairs = get_pair(ids)\n",
    "            pair = max(pairs, key=pairs.get)\n",
    "            new_id = 256 + i\n",
    "\n",
    "            ids = merge(ids, pair, new_id)\n",
    "            self.merges[pair] = new_id\n",
    "            self.vocabulary[new_id] = (\n",
    "                self.vocabulary[pair[0]] + self.vocabulary[pair[1]]\n",
    "            )\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Compression ratio:\", initial_len / len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c97bb2-cb54-4982-bbb3-03acc671992e",
   "metadata": {},
   "source": [
    "## 7. Encoding New Text\n",
    "\n",
    "At inference time, we **must apply merges in the same order they were learned**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01b73d17-e6c9-4702-84b8-1dd2c8dfcd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(self, text):\n",
    "    ids = list(text.encode(\"utf-8\"))\n",
    "\n",
    "    while len(ids) > 1:\n",
    "        pairs = get_pair(ids)\n",
    "        pair = min(pairs, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "        if pair not in self.merges:\n",
    "            break\n",
    "        ids = merge(ids, pair, self.merges[pair])\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba69c7-1821-4ea4-a8b9-a7cfb8fd20d2",
   "metadata": {},
   "source": [
    "## 8. Decoding Tokens Back to Text\n",
    "\n",
    "Tokenization must be reversible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f72afb62-0f5f-4e1f-8c58-77ef6503049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(self, ids):\n",
    "        byte_string = b\"\".join(self.vocabulary[i] for i in ids)\n",
    "        return byte_string.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce61c8a7-5245-405b-b81b-27b4a72c193d",
   "metadata": {},
   "source": [
    "## 9. Let's Practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a50e9-2120-4b2c-86b4-76c435aee0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocabulary = {i: bytes([i]) for i in range(256)}\n",
    "        self.merges = {}\n",
    "\n",
    "    def train(self, text, verbose=False):\n",
    "        assert self.vocab_size > 256\n",
    "\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "        initial_len = len(ids)\n",
    "\n",
    "        for i in range(self.vocab_size - 256):\n",
    "            pairs = get_pair(ids)\n",
    "            pair = max(pairs, key=pairs.get)\n",
    "            new_id = 256 + i\n",
    "\n",
    "            ids = merge(ids, pair, new_id)\n",
    "            self.merges[pair] = new_id\n",
    "            self.vocabulary[new_id] = (\n",
    "                self.vocabulary[pair[0]] + self.vocabulary[pair[1]]\n",
    "            )\n",
    "            #print(sorted( [(v, k) for k,v in pairs.items()], reverse = True) [:10])\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Compression ratio:\", initial_len / len(ids))\n",
    "            \n",
    "    def encode(self, text):\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "    \n",
    "        while len(ids) > 1:\n",
    "            pairs = get_pair(ids)\n",
    "            pair = min(pairs, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            ids = merge(ids, pair, self.merges[pair])\n",
    "    \n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        byte_string = b\"\".join(self.vocabulary[i] for i in ids)\n",
    "        return byte_string.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e14d3bf5-37ef-4d63-b15f-7cb930f11af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text to encode is the following: hello ğŸ˜„ students\n",
      "Compression ratio: 2.111111111111111\n",
      "[265, 115, 116, 117, 100, 101, 110, 116, 115]\n",
      "hello ğŸ˜„ students\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BasicTokenizer(266)\n",
    "print(\"The text to encode is the following:\", text)\n",
    "tokenizer.train(text, verbose=True)\n",
    "\n",
    "encoded = tokenizer.encode(\"hello ğŸ˜„ students\")\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee801642-77ac-43e3-b676-1b4d88ad18af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text to encode is the following: hello everyone\n",
      "Compression ratio: 3.5\n",
      "[265, 111, 110, 101]\n",
      "hello everyone\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BasicTokenizer(266)\n",
    "textTest = \"hello everyone\"\n",
    "print(\"The text to encode is the following:\", textTest)\n",
    "tokenizer.train(textTest, verbose=True)\n",
    "\n",
    "encoded = tokenizer.encode(textTest)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b4df3-b4dd-4664-b289-047a5d6abe58",
   "metadata": {},
   "source": [
    "## 10. Now , your turn!\n",
    "\n",
    "Try the following:\n",
    "\n",
    "1. Change `vocab_size` and measure compression\n",
    "2. Train on:\n",
    "   - English text\n",
    "   - Code\n",
    "   - Emojis\n",
    "3. Print the first 20 learned merges\n",
    "4. Compare with character-level tokenization\n",
    "\n",
    "âœï¸ Write short answers below each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30acfdb6-92f2-41e8-bffc-386fca470397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text to encode is the following: âœï¸ Write short answers below each experiment.\n",
      "Compression ratio: 2.130434782608696\n",
      "[279, 114, 115, 32, 98, 101, 108, 111, 119, 257, 97, 99, 104, 257, 120, 112, 101, 256, 109, 101, 110, 116, 46]\n",
      "âœï¸ Write short answers below each experiment.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BasicTokenizer(280)\n",
    "textTest = \"âœï¸ Write short answers below each experiment.\"\n",
    "print(\"The text to encode is the following:\", textTest)\n",
    "tokenizer.train(textTest, verbose=True)\n",
    "\n",
    "encoded = tokenizer.encode(textTest)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951c123-8399-4e6d-a34f-4a1c55e8a843",
   "metadata": {},
   "source": [
    "## 10. Why This Matters for Language Models\n",
    "\n",
    "- Tokens â†’ embeddings\n",
    "- Fewer tokens â†’ longer context\n",
    "- Tokenization is the **first inductive bias** of a Transformer\n",
    "- Good tokenization matters more when the model is small.\n",
    "\n",
    "This tokenizer can now be plugged directly into a GPT training loop.\n",
    "\n",
    "<a href=\"https://tiktokenizer.vercel.app/\" >\n",
    "    <img src=\"Tiktokenizer.png\" >\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08948b-4e9e-4ed5-9134-ef1ea162e637",
   "metadata": {},
   "source": [
    "# Regex Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3920c2e-7761-4b4c-b970-4dc72b884e10",
   "metadata": {},
   "source": [
    "### 1. Why RegexTokenizer Exists\n",
    "\n",
    "Your current basicTokenizer trains BPE on raw byte streams:\n",
    "\n",
    "```\n",
    "text â†’ bytes â†’ BPE merges\n",
    "```\n",
    "\n",
    "That works â€” but it has drawbacks:\n",
    "\n",
    "- BPE may merge across semantic boundaries\n",
    "\n",
    "- Punctuation, whitespace, and numbers get mixed\n",
    "\n",
    "- Training is slower and noisier\n",
    "\n",
    "- Tokens can become syntactically awkward\n",
    "\n",
    "RegexTokenizer fixes this by adding structure before BPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4e824-1f80-4dd1-8950-5d5d9d7e7705",
   "metadata": {},
   "source": [
    "### 2ï¸. Core Idea (One Sentence)\n",
    "\n",
    "RegexTokenizer first splits text into meaningful chunks using regex, then applies BPE inside each chunk independently.\n",
    "\n",
    "This is exactly how GPT-2 / GPT-3 style tokenizers work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3786db34-773b-421b-af0d-753eb716dd30",
   "metadata": {},
   "source": [
    "### 3. High-Level Pipeline\n",
    "\n",
    "```\n",
    "Text\n",
    " â†“\n",
    "Regex split (words, numbers, punctuation, spaces)\n",
    " â†“\n",
    "Each chunk â†’ UTF-8 bytes\n",
    " â†“\n",
    "Byte Pair Encoding (BPE)\n",
    " â†“\n",
    "Final token IDs\n",
    "```\n",
    "\n",
    "So instead of training BPE on everything, we train it on pre-segmented text units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b734a-2a3f-47dd-9d9a-31c7dd888c86",
   "metadata": {},
   "source": [
    "Letâ€™s decode what it does.\n",
    "\n",
    "What the Regex Captures\n",
    "Pattern\tMeaning\n",
    "\n",
    "| Pattern    | Meaning |\n",
    "| -------- | ------- |\n",
    "| \\p{L}+  | Letters (words)    |\n",
    "| \\p{N}+ | Numbers     |\n",
    "| [^ \\s\\p{L}\\p{N}]+    | Punctuation / symbols    |\n",
    "| \\s+    | Whitespace    |\n",
    "| 's, 't, etc.    | English contractions   |\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04d5a989-c47f-4510-8d2c-fafb3e78c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "pattern = re.compile(\n",
    "    r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\", # NOTE: this split pattern deviates from GPT-4 in that it is used \\p{N}{1,2} instead of \\p{N}{1,3}\n",
    "# I did this because I didn't want to \"waste\" too many tokens on numbers for smaller vocab sizes.\n",
    "# I haven't validated that this is actually a good idea, TODO.\n",
    "    re.UNICODE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18946319-155a-432c-a7df-f3ca25364529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ğŸ˜„ students\n",
      "--------------------------------------------------\n",
      "Splitting Regex Pattern Capture:\n",
      " ['Hello', ' ğŸ˜„', ' students']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello ğŸ˜„ students\"\n",
    "print(text)\n",
    "print(\"-\"*50)\n",
    "print(\"Splitting Regex Pattern Capture:\\n\", re.findall(pattern, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b7041-112d-4265-b51f-d3a852671006",
   "metadata": {},
   "source": [
    "#### ! Important: Spaces are kept, not discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eccfc9f1-c1e0-4d10-a144-a1978a5378d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ï¼µï½ï½‰ï½ƒï½ï½„ï½…',\n",
       " '!',\n",
       " ' ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½',\n",
       " ' ğŸ‡º\\u200cğŸ‡³\\u200cğŸ‡®\\u200cğŸ‡¨\\u200cğŸ‡´\\u200cğŸ‡©\\u200cğŸ‡ª!',\n",
       " ' ğŸ˜„',\n",
       " ' The',\n",
       " ' very',\n",
       " ' name',\n",
       " ' strikes',\n",
       " ' fear',\n",
       " ' and',\n",
       " ' awe',\n",
       " ' into',\n",
       " ' the',\n",
       " ' hearts',\n",
       " ' of',\n",
       " ' programmers',\n",
       " ' worldwide',\n",
       " '.',\n",
       " ' We',\n",
       " ' all',\n",
       " ' know',\n",
       " ' we',\n",
       " ' ought',\n",
       " ' to',\n",
       " ' â€œ',\n",
       " 'support',\n",
       " ' Unicode',\n",
       " 'â€',\n",
       " ' in',\n",
       " ' our',\n",
       " ' software',\n",
       " ' (',\n",
       " 'whatever',\n",
       " ' that',\n",
       " ' means',\n",
       " 'â€”like',\n",
       " ' using',\n",
       " ' wchar',\n",
       " '_t',\n",
       " ' for',\n",
       " ' all',\n",
       " ' the',\n",
       " ' strings',\n",
       " ',',\n",
       " ' right',\n",
       " '?).',\n",
       " ' But',\n",
       " ' Unicode',\n",
       " ' can',\n",
       " ' be',\n",
       " ' abstruse',\n",
       " ',',\n",
       " ' and',\n",
       " ' diving',\n",
       " ' into',\n",
       " ' the',\n",
       " ' thousand',\n",
       " '-page',\n",
       " ' Unicode',\n",
       " ' Standard',\n",
       " ' plus',\n",
       " ' its',\n",
       " ' dozens',\n",
       " ' of',\n",
       " ' supplementary',\n",
       " ' annexes',\n",
       " ',',\n",
       " ' reports',\n",
       " ',',\n",
       " ' and',\n",
       " ' notes',\n",
       " ' can',\n",
       " ' be',\n",
       " ' more',\n",
       " ' than',\n",
       " ' a',\n",
       " ' little',\n",
       " ' intimidating',\n",
       " '.',\n",
       " ' I',\n",
       " ' don',\n",
       " 'â€™t',\n",
       " ' blame',\n",
       " ' programmers',\n",
       " ' for',\n",
       " ' still',\n",
       " ' finding',\n",
       " ' the',\n",
       " ' whole',\n",
       " ' thing',\n",
       " ' mysterious',\n",
       " ',',\n",
       " ' even',\n",
       " ' ',\n",
       " '30',\n",
       " ' years',\n",
       " ' after',\n",
       " ' Unicode',\n",
       " 'â€™s',\n",
       " ' inception',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "print(text)\n",
    "print(\"-\"*50)\n",
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e76d5-ef98-478f-9c22-4cc08d819698",
   "metadata": {},
   "source": [
    "### 3. Why This Matters Before BPE\n",
    "Without Regex (Your Basic Tokenizer)\n",
    "\n",
    "BPE might merge:\n",
    "\n",
    "```\n",
    "\n",
    "\"o \" + \"t\" â†’ \"o t\"\n",
    "\n",
    "```\n",
    "\n",
    "Which is meaningless.\n",
    "\n",
    "With RegexTokenizer\n",
    "\n",
    "BPE operates inside units like:\n",
    "\n",
    "- \"hello\"\n",
    "\n",
    "- \" there\"\n",
    "\n",
    "- \"!\"\n",
    "\n",
    "This ensures:\n",
    "\n",
    "- Cleaner merges\n",
    "\n",
    "- Faster convergence\n",
    "\n",
    "- More interpretable tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20e4a9b4-3de3-43a2-9231-50a41ff9aec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ' there',\n",
       " '!',\n",
       " ' I',\n",
       " \"'m\",\n",
       " ' ',\n",
       " '26',\n",
       " '.',\n",
       " ' ğŸ˜„\\n',\n",
       " 'New',\n",
       " ' line',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"Hello there! I'm 26. ğŸ˜„\\nNew line.\"\n",
    "chunks = re.findall(pattern, sample)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "478d0af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a' a\n",
      "[240, 159, 152, 132]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {i: bytes([i]) for i in range(256)}\n",
    "print(vocabulary[97], vocabulary[97].decode(\"utf-8\"))  # 'a'\n",
    "print(list(\"ğŸ˜„\".encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ec971",
   "metadata": {},
   "source": [
    "### BPE primitives: count adjacent pairs + merge\n",
    "\n",
    "BPE repeatedly merges the most frequent adjacent pair into a new token id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c2cea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair(ids, counts):\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, index):\n",
    "    '''\n",
    "    This function will iterate over ids and every time\n",
    "    it sees a instance of pair, it will take that pair\n",
    "    and instead put index , then it will return the list\n",
    "    merge()\n",
    "    list = [1,2, 3, 4, 1, 2]\n",
    "    merge(list, (1,2). 257)\n",
    "    list = [257, 3, 4, 257, 3]\n",
    "    '''\n",
    "\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i <len(ids) - 1 and  (ids[i], ids[i+1]) == pair:\n",
    "            new_ids.append(index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb8fcf",
   "metadata": {},
   "source": [
    "### 4. RegexTokenizer overview\n",
    "\n",
    "Training:\n",
    "1. Split text into regex chunks\n",
    "2. Convert each chunk to UTF-8 bytes (list of ints)\n",
    "3. Count pairs across **all chunks**\n",
    "4. Merge the most frequent pair across **all chunks**\n",
    "5. Repeat until reaching vocab_size\n",
    "\n",
    "Encoding:\n",
    "1. Split input text into regex chunks\n",
    "2. Encode each chunk with learned merges\n",
    "3. Concatenate token ids\n",
    "\n",
    "Decoding:\n",
    "- Map ids â†’ bytes â†’ UTF-8 string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c120ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08d8af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class regexTokenizer:\n",
    "\n",
    "    def __init__(self, vocab_size, pattern):\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocabulary = {i : bytes([i]) for i in range(256)}\n",
    "        self.merges = {}\n",
    "        self.pattern = re.compile(pattern)\n",
    "\n",
    "    def train(self, text, verbose = False):\n",
    "        # Encode the text\n",
    "        # Iterate over text, self.vocab_size - 256 times\n",
    "        # count all of the pairs in a dictionary\n",
    "        # choose the pair with the highest frequency\n",
    "        # merge that pair as a new token\n",
    "        # add that token to the vocab\n",
    "        # {256: byte_string}\n",
    "        # add to self.merges = {byte_string: 256}\n",
    "        \n",
    "        assert self.vocab_size > 256\n",
    "        number_merges = self.vocab_size - 256\n",
    "\n",
    "        text_chunks = re.findall(self.pattern, text)\n",
    "        encoded_chunks = [list(text_chunk.encode('utf-8')) for text_chunk in text_chunks]\n",
    "\n",
    "        length_initial = sum([len(encoded_chunk) for encoded_chunk in encoded_chunks])\n",
    "\n",
    "        for i in range(number_merges):\n",
    "            pairs = {}\n",
    "            for encoded_chunk in encoded_chunks:\n",
    "                pairs = get_pair(encoded_chunk, pairs)\n",
    "\n",
    "            pair = max(pairs, key = pairs.get)\n",
    "            index = 256 + i\n",
    "            encoded_chunks = [merge(encoded_chunk,pair,index) for encoded_chunk in encoded_chunks]\n",
    "            self.merges[pair] = index\n",
    "            self.vocabulary[index] = self.vocabulary[pair[0]] + self.vocabulary[pair[1]]\n",
    "            #print(sorted( [(v, k) for k,v in pairs.items()], reverse = True) [:10])\n",
    "\n",
    "        if verbose:\n",
    "            length_final = sum([len(encoded_chunk) for encoded_chunk in encoded_chunks])\n",
    "            compression = length_initial/length_final\n",
    "            print(length_initial, length_final)\n",
    "            print(compression)\n",
    "\n",
    "    def encode(self, text):\n",
    "\n",
    "        text_chunks = re.findall(self.pattern, text)\n",
    "        encoded_text = []\n",
    "\n",
    "        for text_chunk in text_chunks:\n",
    "            encoded_chunk = self.encode_chunk(text_chunk)\n",
    "            encoded_text.extend(encoded_chunk)\n",
    "        return encoded_text\n",
    "\n",
    "    def encode_chunk(self, text):\n",
    "        '''\n",
    "        self.merges is important here\n",
    "        \n",
    "        we get text, and then we convert that text to byte strings, then to integers\n",
    "        and then we iterate over the text until all pairs of\n",
    "        merges that are possible under the trained tokenizer\n",
    "        have been completed\n",
    "\n",
    "        '''\n",
    "\n",
    "        ids = list(text.encode('utf-8'))\n",
    "        \n",
    "        for pair, index in self.merges.items():\n",
    "            ids = merge(ids, pair, index)\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        '''\n",
    "        decode gets ids \n",
    "        1. convert the ids to their byte strings\n",
    "        2. convert the byte strings to strings via the vocabulary\n",
    "        3. then return the decoded_text\n",
    "        '''\n",
    "\n",
    "        byte_strings = b''.join([bytes(self.vocabulary[i]) for i in ids])\n",
    "        decoded_text =  byte_strings.decode('utf-8')\n",
    "        return decoded_text\n",
    "    \n",
    "    def save(self, path):\n",
    "        with open(path, \"wb\") as file:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    \"merges\": self.merges,\n",
    "                    \"vocabulary\": self.vocabulary,\n",
    "                    \"pattern\": self.pattern\n",
    "                },\n",
    "                file\n",
    "            )\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        tokenizer = cls(300, pattern)\n",
    "\n",
    "        with open(path , \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "            tokenizer.merges = data[\"merges\"]\n",
    "            tokenizer.vocabulary = data[\"vocabulary\"]\n",
    "            tokenizer.pattern = data[\"pattern\"]\n",
    "        return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "389956f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = regexTokenizer(300, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "411425bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616 383\n",
      "1.608355091383812\n"
     ]
    }
   ],
   "source": [
    "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "tokenizer.train(text, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e67d48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "panel-cell-order": [
   "f01e1d45-3aab-465c-9480-fc7dc5106e69",
   "306cd8b2-2a76-422a-b78c-f181e1a64620",
   "bdae7223-9d36-4c45-93a2-bf4fd6a74bcd"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
