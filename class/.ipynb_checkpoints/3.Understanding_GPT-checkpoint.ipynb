{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ec2e79-deab-4431-975e-4bf00540cd56",
   "metadata": {},
   "source": [
    "## Understanding Transformers and GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6206130-34b8-4d3b-ac31-8b63a9e2615d",
   "metadata": {},
   "source": [
    "## Transformers 101 (Hands-On)\n",
    "\n",
    "Goal:\n",
    "- Understand the Transformer pipeline using a pretrained model\n",
    "- Run inference on a real dataset\n",
    "- (Optional) fine-tune for a few steps\n",
    "\n",
    "We will use:\n",
    "- Dataset: AG News (4-class news classification)\n",
    "- Model: DistilBERT (pretrained Transformer encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c25eba-78a5-43b5-8842-a4a522782baf",
   "metadata": {},
   "source": [
    "### 1.Install & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51141f4a-1158-4736-8ac2-0c1cf8201753",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers datasets accelerate evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca26224-99bb-4be8-a07f-63402f260c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46044877-59bb-4de7-971b-dc9fb1f655d4",
   "metadata": {},
   "source": [
    "### 2.Dataset Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "189585b9-5a15-48c6-a0fa-a5be753511e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f6b5ad66274e8587fe92740e428f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2026-01-12 07:31:26,329 - huggingface_hub.file_download - \u001b[33m\u001b[1mWARNING\u001b[0m - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5686b4d05f97457ea8008ca7cd5a1eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2026-01-12 07:31:28,394 - huggingface_hub.file_download - \u001b[33m\u001b[1mWARNING\u001b[0m - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91577feb57649a0a794cf03f47bfbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a3fb19059c4f7d98045e82eac58994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d69e7397bc4f6fadd20205e5fdb991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"ag_news\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc8ab1c-2338-4ed2-9e05-aa56ea263aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect a sample\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82eeeb9e-6008-4a97-86da-a76251b8a8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['World', 'Sports', 'Business', 'Sci/Tech']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = dataset[\"train\"].features[\"label\"].names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7273be-3278-47b2-8b8b-2c1420ff8e94",
   "metadata": {},
   "source": [
    "### 3.Load Tokenizer + Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "195e1ef4-53d5-4617-85f2-6d83e3034168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33ce6f4cec8482486b63018fcf81190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1236f85d66754b1aac4cae0995634e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/754 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b73cf5067ec40b0b425c6a7781d74ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3943ce7a6a7b451eb60e3cf851d4f008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08581bdd0916459e8cadb76606e41895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2026-01-12 07:45:43,240 - huggingface_hub.file_download - \u001b[33m\u001b[1mWARNING\u001b[0m - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a5bb6a1cb64f8c9a7c458ef092970f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-ag-news were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2026-01-12 07:46:41,323 - huggingface_hub.file_download - \u001b[33m\u001b[1mWARNING\u001b[0m - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c644475bfb640a092723a758d9ada1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"textattack/roberta-base-ag-news\" #0.94 accuracy on AG News Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=4\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "851be801-8fee-40f4-9688-5f5a87d02be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base-ag-news\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=4\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de7e52-ac51-4967-a897-c25a1d27bc9d",
   "metadata": {},
   "source": [
    "### 4. Tokenization: Text → Input IDs + Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d0d820e-3133-4559-9a28-6cf1236d7dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 20770, 36685,  5290,    10,    92,  2257,   806,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Apple unveils a new software technology\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0aa8c66-4879-4567-bb48-c94e8669a6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Apple',\n",
       " 'Ġunve',\n",
       " 'ils',\n",
       " 'Ġa',\n",
       " 'Ġnew',\n",
       " 'Ġsoftware',\n",
       " 'Ġtechnology',\n",
       " '</s>']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviewing Tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2032fc4e-a353-45a5-855d-692e97cb40ba",
   "metadata": {},
   "source": [
    "## 5. Inference: Get Logits -> Probabilities -> Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5316d05f-ed80-4749-9bd3-99e4a02a0b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Apple unveils a new software technology\n",
      "Predicted label: 3 -> Sci/Tech\n",
      "Probabilities: [0.0055228122510015965, 3.904529512510635e-05, 0.009389317594468594, 0.9850488305091858]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "pred_id = probs.argmax(dim=-1).item()\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Predicted label:\", pred_id, \"->\", label_names[pred_id])\n",
    "print(\"Probabilities:\", probs.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "443a16d1-8f9b-45b2-8bdc-a12ff4aefbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Text: Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul. ...\n",
      "True: 2 Business\n",
      "Pred: 2 Business\n",
      "================================================================================\n",
      "Text: The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com) SPACE.com - TORONTO, Canada -- A second\\team of rocketeers competing for the  #36;10 million Ansari X Prize, a co ...\n",
      "True: 3 Sci/Tech\n",
      "Pred: 3 Sci/Tech\n",
      "================================================================================\n",
      "Text: Ky. Company Wins Grant to Study Peptides (AP) AP - A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are s ...\n",
      "True: 3 Sci/Tech\n",
      "Pred: 3 Sci/Tech\n",
      "================================================================================\n",
      "Text: Prediction Unit Helps Forecast Wildfires (AP) AP - It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day w ...\n",
      "True: 3 Sci/Tech\n",
      "Pred: 3 Sci/Tech\n",
      "================================================================================\n",
      "Text: Calif. Aims to Limit Farm-Related Smog (AP) AP - Southern California's smog-fighting agency went after emissions of the bovine variety Friday, adopting the nation's first rules to reduce air pollution ...\n",
      "True: 3 Sci/Tech\n",
      "Pred: 3 Sci/Tech\n",
      "================================================================================\n",
      "Text: Open Letter Against British Copyright Indoctrination in Schools The British Department for Education and Skills (DfES) recently launched a \"Music Manifesto\" campaign, with the ostensible intention of  ...\n",
      "True: 3 Sci/Tech\n",
      "Pred: 3 Sci/Tech\n",
      "================================================================================\n",
      "Text: Loosing the War on Terrorism \\\\\"Sven Jaschan, self-confessed author of the Netsky and Sasser viruses, is\\responsible for 70 percent of virus infections in 2004, according to a six-month\\virus roundup  ...\n",
      "True: 3 Sci/Tech\n",
      "Pred: 3 Sci/Tech\n",
      "================================================================================\n",
      "Text: FOAFKey: FOAF, PGP, Key Distribution, and Bloom Filters \\\\FOAF/LOAF  and bloom filters have a lot of interesting properties for social\\network and whitelist distribution.\\\\I think we can go one level  ...\n",
      "True: 3 Sci/Tech\n",
      "Pred: 3 Sci/Tech\n"
     ]
    }
   ],
   "source": [
    "batch_texts = [dataset[\"test\"][i][\"text\"] for i in range(8)]\n",
    "batch_labels = [dataset[\"test\"][i][\"label\"] for i in range(8)]\n",
    "\n",
    "batch_inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**batch_inputs).logits\n",
    "    preds = logits.argmax(dim=-1).tolist()\n",
    "\n",
    "for i, (t, y, p) in enumerate(zip(batch_texts, batch_labels, preds)):\n",
    "    print(\"=\"*80)\n",
    "    print(\"Text:\", t[:200], \"...\")\n",
    "    print(\"True:\", y, label_names[y])\n",
    "    print(\"Pred:\", p, label_names[p])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72745ff8-9dd8-4552-bc8f-def556ee86e6",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e79f8fb-e17d-435f-9975-fe74519ec090",
   "metadata": {},
   "source": [
    "\n",
    "- Load a pretrained GPT-2 model and tokenizer\n",
    "- See how text becomes token IDs\n",
    "- Run a forward pass to get logits (next-token distribution)\n",
    "- Compute causal language modeling loss\n",
    "- Generate text with sampling\n",
    "\n",
    "We use:\n",
    "- Model: distilgpt2 (fast), optionally gpt2\n",
    "- Dataset: WikiText-2 (free)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d436cb-29d9-4bef-a896-1baaf736b6b0",
   "metadata": {},
   "source": [
    "## 1. Install and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c87b2c9a-63af-47c7-b39c-e3d4376b16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "536f13a6-0356-4c12-ab09-e31e2441c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9dbfa-1f93-4709-80bc-1347481da61b",
   "metadata": {},
   "source": [
    "## 2. Load Tokenizer + Pretrained GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87125718-ab13-4c45-b7f9-aed16370824d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9bb5417bd24bd7898f6f5cd5bb6278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c748261c4a74d94ba2886e0b08d90bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa3e4f5d9e24897a7b0f2d9e8c0da78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16523ce1cc5240fabb8e37a16663e51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a82e7edcd284dbc96fc28f88ffb0f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2026-01-12 09:00:36,620 - huggingface_hub.file_download - \u001b[33m\u001b[1mWARNING\u001b[0m - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99737ceab1ad462b964c0cbbe2ebdde2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2340ac0e31654aeaa47b89e6a3767bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"  # change to \"gpt2\" if you have GPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Previous step because the tokenizer does not have a pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea434b32-ac22-40bf-8463-45d56c528df9",
   "metadata": {},
   "source": [
    "## 3. Tokenization: Text → Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff321b41-566d-4e5c-b4bd-4966ae75a083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Transformers are amazing because\n",
      "input_ids shape: torch.Size([1, 5])\n",
      "Token IDs: [41762, 364, 389, 4998, 780]\n",
      "Tokens: ['Transform', 'ers', 'Ġare', 'Ġamazing', 'Ġbecause']\n"
     ]
    }
   ],
   "source": [
    "text = \"Transformers are amazing because\"\n",
    "enc = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = enc[\"input_ids\"].to(device)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"input_ids shape:\", input_ids.shape)\n",
    "print(\"Token IDs:\", input_ids[0].tolist())\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(input_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169227d1-ce4c-4980-9f9f-20007b15799a",
   "metadata": {},
   "source": [
    "## 4. Forward Pass: Logits (Next-token Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0293d5d-3372-488e-a740-3b6f6dccc3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([1, 5, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(input_ids=input_ids)\n",
    "\n",
    "logits = out.logits  # (B, T, V)\n",
    "print(\"logits shape:\", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "485190f2-e109-4704-a418-1679eaea4399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 next token predictions:\n",
      "' they'       p=0.4544\n",
      "' of'         p=0.0585\n",
      "' the'        p=0.0507\n",
      "' it'         p=0.0465\n",
      "' you'        p=0.0405\n",
      "' we'         p=0.0372\n",
      "' their'      p=0.0250\n",
      "' when'       p=0.0186\n",
      "' there'      p=0.0165\n",
      "' I'          p=0.0152\n"
     ]
    }
   ],
   "source": [
    "# Next Token Probabilities\n",
    "last_logits = logits[0, -1]  # (V,)\n",
    "probs = F.softmax(last_logits, dim=-1)\n",
    "\n",
    "topk = torch.topk(probs, k=10)\n",
    "top_ids = topk.indices.tolist()\n",
    "top_probs = topk.values.tolist()\n",
    "\n",
    "print(\"Top-10 next token predictions:\")\n",
    "for tid, p in zip(top_ids, top_probs):\n",
    "    print(f\"{tokenizer.decode([tid])!r:12s}  p={p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "328489c0-b0e3-438e-9697-188bbce29d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.066333293914795\n"
     ]
    }
   ],
   "source": [
    "# Loss Function of our model using the previous inputs_ids. This is self-supervised learning: the text supplies its own labels.\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=input_ids, labels=input_ids)\n",
    "\n",
    "print(\"Loss:\", out.loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0822421-75a2-43a1-8ab2-2b52184e3a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t | input token -> target token\n",
      " 0 | 'Transform'     -> 'ers'\n",
      " 1 | 'ers'           -> 'Ġare'\n",
      " 2 | 'Ġare'          -> 'Ġamazing'\n",
      " 3 | 'Ġamazing'      -> 'Ġbecause'\n"
     ]
    }
   ],
   "source": [
    "ids = input_ids[0].tolist()\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "print(\"t | input token -> target token\")\n",
    "for t in range(min(len(tokens)-1, 12)):\n",
    "    print(f\"{t:2d} | {tokens[t]!r:15s} -> {tokens[t+1]!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c033c-04d8-4cbf-bf2f-6ea9618ed35a",
   "metadata": {},
   "source": [
    "## 5. We have two ways to see the results (Greedy vs Probabilistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "edf6e8c8-df77-41e6-af65-67e895a7a672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is to be understood as a whole, and to be understood as a whole, and to be understood as a whole, and to be understood as a whole, and to be understood as a whole, and\n"
     ]
    }
   ],
   "source": [
    "## Greedy\n",
    "prompt = \"The meaning of life is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "greedy = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(greedy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8fe97252-61b8-4f0c-8a83-de20e8f84228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is to be able to be able to be able to be able to be able to be able to be able to be able to\n"
     ]
    }
   ],
   "source": [
    "## Probabilistic or Sampling\n",
    "sampled = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=25,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampled[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1df1af-2070-425c-b522-c53968ac7160",
   "metadata": {},
   "source": [
    "## HOLD ON!! WHAT IS TEMPERATURE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1470669f-9444-4e05-9e45-b35121470fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is that it changes when an external change happens. This applies not only in our personal life to an external life in this specific country\n"
     ]
    }
   ],
   "source": [
    "## Probabilistic or Sampling\n",
    "sampled = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=25,\n",
    "    do_sample=True,\n",
    "    temperature=2.0,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(sampled[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e6d88-3cb1-48e5-9eff-d4057362d098",
   "metadata": {},
   "source": [
    "## 6. Conclusions\n",
    "\n",
    "- GPT-2 is a decoder-only Transformer trained with next-token prediction.\n",
    "- Tokenization produces `input_ids`.\n",
    "- The model outputs `logits` with shape (batch, time, vocab).\n",
    "- Causal LM loss predicts token t+1 from tokens up to t.\n",
    "- Generation is iterative: forward pass → sample next token → append.\n",
    "- Fine-tuning adapts the model quickly with a small dataset subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03d86a-a5a6-4789-a844-fbfc9ac143ee",
   "metadata": {},
   "source": [
    "# Let's start with: Nanochat GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "547c2c77-3802-421d-bed4-e19e87183e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanochat.gpt import GPT, GPTConfig\n",
    "from nanochat.tokenizer import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa35d8-0ddc-4064-9101-0b06fd78db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    n_kv_head=4,\n",
    "    n_embd=256,\n",
    ")\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb6abf-251b-4269-9200-03288752cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nanochat has almost {round(sum(p.numel() for p in model.parameters()) /1e6, 2)} MM parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b892d62-3d14-4e46-9cdf-d375205b3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is a transformer?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"A transformer is a neural network based on attention.\"}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939148b8-d709-4289-b7b8-10bf54457e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, loss_mask = tokenizer.render_conversation(conversation)\n",
    "\n",
    "print(\"Number of tokens:\", len(ids))\n",
    "print(\"Loss tokens:\", sum(loss_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d72c5-4bfa-49c2-bf3d-2f6512484700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = torch.tensor(ids).unsqueeze(0)  # (1, T)\n",
    "logits = model(input_ids)\n",
    "\n",
    "loss_mask_t = torch.tensor(loss_mask).unsqueeze(0)\n",
    "\n",
    "targets = input_ids[:, 1:]\n",
    "logits = logits[:, :-1, :]\n",
    "\n",
    "loss_mask_t = loss_mask_t[:, 1:]\n",
    "\n",
    "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "target_log_probs = log_probs.gather(-1, targets.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "loss = -(target_log_probs * loss_mask_t).sum() / loss_mask_t.sum()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cba921-a759-43c8-83e1-3022fe49855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for step in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_ids)\n",
    "    \n",
    "    logits = logits[:, :-1, :]\n",
    "    targets = input_ids[:, 1:]\n",
    "    mask = loss_mask_t\n",
    "    \n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    loss = -(log_probs.gather(-1, targets.unsqueeze(-1)).squeeze(-1) * mask).sum() / mask.sum()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad0f2a1-0245-4ed4-bab8-e7d234ea79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanochat.engine import Engine\n",
    "\n",
    "engine = Engine(model, tokenizer)\n",
    "\n",
    "print(\"Model output:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "input_ids = tokenizer.render_for_completion(conversation)\n",
    "\n",
    "output = \"\"\n",
    "for token_ids, token_mask in engine.generate(\n",
    "    input_ids,\n",
    "    max_tokens=50,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "):\n",
    "    text_piece = tokenizer.decode(token_ids)\n",
    "    print(text_piece, end=\"\", flush=True)\n",
    "    output += text_piece\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
